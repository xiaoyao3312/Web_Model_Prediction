# C:\Users\user\Desktop\Web_Model_Prediction\projects\Churn_Bank_code\churn_bank_train.py
# éŠ€è¡Œå®¢æˆ¶æµå¤±é æ¸¬ - XGBoost Optuna/SHAP æ•´åˆç‰ˆ (è¨“ç·´éƒ¨åˆ†)

import logging
import warnings
import argparse
import sys
import os 
from typing import Any, Callable, Tuple, Dict, List
import joblib 

# è¨­ç½®è­¦å‘Šå’Œæ—¥èªŒ
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('MainScript')

# æª¢æŸ¥å¿…è¦çš„åº«æ˜¯å¦å·²å®‰è£
try:
    import numpy as np
    import pandas as pd
    from xgboost import XGBClassifier
    from sklearn.model_selection import StratifiedKFold
    from sklearn.metrics import roc_auc_score
    from sklearn.base import clone
    import optuna
except ImportError as e:
    print(f"éŒ¯èª¤: ç¼ºå°‘å¿…è¦çš„åº«ã€‚è«‹åŸ·è¡Œ pip install numpy pandas xgboost optuna scikit-learn shap: {e}")
    sys.exit(1)


# --- é…ç½® ---
class Config:
    TARGET_COL = 'Exited'
    N_SPLITS = 5
    RANDOM_STATE = 42
    # æ¨¡å‹å°‡è¼¸å‡ºåˆ°ç•¶å‰åŸ·è¡Œç›®éŒ„ (å³ CWD)
    MODEL_DIR = './' 
    FE_PIPELINE_FILE = os.path.join(MODEL_DIR, 'feature_engineer_pipeline.joblib')

# --- ç‰¹å¾µå·¥ç¨‹é¡åˆ¥ (FeatureEngineer) ---
class FeatureEngineer:
    """
    ç”¨æ–¼ç‰¹å¾µå·¥ç¨‹çš„å·¥å…·é¡åˆ¥ã€‚
    """
    @staticmethod
    def map_columns(df: pd.DataFrame, mappings: dict) -> pd.DataFrame:
        df_copy = df.copy()
        for col, mapping in mappings.items():
            if col in df_copy.columns:
                df_copy[col] = df_copy[col].map(mapping)
        return df_copy

    @staticmethod
    def cast_columns(df: pd.DataFrame, int_cols: Any = None, 
                     cat_cols: Any = None) -> pd.DataFrame:
        df_copy = df.copy()
        if int_cols:
            for col in int_cols:
                if col in df_copy.columns:
                    # é¿å…è½‰æ› NaNï¼Œå¡«å……ç‚º 0
                    df_copy[col] = df_copy[col].fillna(0).astype(int) 
        return df_copy

    @staticmethod
    def run_v1_preprocessing(df: pd.DataFrame, is_train: bool) -> pd.DataFrame:
        df_copy = df.copy()
        gender_map = {'Male': 0, 'Female': 1}
        
        # 1. è™•ç† Gender: å°‡ 'Male'/'Female' è½‰æ›ç‚º 0/1
        df_copy = FeatureEngineer.map_columns(df_copy, {'Gender': gender_map}) 
        
        # â­ ä¿®æ­£ï¼šåœ¨å¼·åˆ¶è½‰æ›ç‚º int ä¹‹å‰ï¼Œå°‡æ˜ å°„å¾Œç”¢ç”Ÿçš„ NaN å¡«å……ç‚º 0ã€‚
        if 'Gender' in df_copy.columns:
            # è¨“ç·´é›†å’Œæ¸¬è©¦é›†éƒ½å¯èƒ½æœ‰ NaN
            df_copy['Gender'] = df_copy['Gender'].fillna(0)
            df_copy['Gender'] = df_copy['Gender'].astype(int) # ç¢ºä¿ Gender é¡å‹æ˜¯ int

        # 2. è™•ç† Geography: ç¢ºä¿å®ƒæ˜¯å­—ä¸²
        if 'Geography' in df_copy.columns and df_copy['Geography'].dtype.name != 'object':
             df_copy['Geography'] = df_copy['Geography'].astype(str)
        
        # å¹´é½¡åˆ†ç®±
        if 'Age' in df_copy.columns:
            df_copy['Age_bin'] = pd.cut(df_copy['Age'], bins=[0, 25, 35, 45, 60, np.inf],
                                     labels=['very_young', 'young', 'mid', 'mature', 'senior']).astype(str)
        else:
            df_copy['Age_bin'] = 'unknown'
        
        # å‰µå»ºåŸºç¤ç‰¹å¾µæ——æ¨™ (ä¿æŒ int é¡å‹)
        # ç¢ºä¿æ¶‰åŠ Balance, IsActiveMember çš„æ¬„ä½å­˜åœ¨
        if 'NumOfProducts' in df_copy.columns:
            df_copy['Is_two_products'] = (df_copy['NumOfProducts'] == 2)
        else:
            df_copy['Is_two_products'] = 0
            
        if 'Geography' in df_copy.columns and 'Gender' in df_copy.columns:
            df_copy['Germany_Female'] = ((df_copy['Geography'] == 'Germany') & (df_copy['Gender'] == 1))
        else:
            df_copy['Germany_Female'] = 0

        if 'Geography' in df_copy.columns and 'IsActiveMember' in df_copy.columns:
            df_copy['Germany_Inactive'] = ((df_copy['Geography'] == 'Germany') & (df_copy['IsActiveMember'] == 0))
        else:
            df_copy['Germany_Inactive'] = 0
            
        if 'Balance' in df_copy.columns:
            df_copy['Has_Zero_Balance'] = (df_copy['Balance'] == 0)
        else:
            df_copy['Has_Zero_Balance'] = 0

        # å° Tenure é€²è¡Œ Log è½‰æ› (ç¢ºä¿ Tenure å­˜åœ¨)
        if 'Tenure' in df_copy.columns:
            df_copy['Tenure_log'] = np.log1p(df_copy['Tenure'])
        else:
            df_copy['Tenure_log'] = 0.0

        # å°‡å¸ƒæ—é¡å‹è½‰æ›ç‚º int
        for col in ['Is_two_products', 'Germany_Female', 'Germany_Inactive', 'Has_Zero_Balance']:
            if col in df_copy.columns:
                 df_copy[col] = df_copy[col].astype(int)

        int_cols = ['HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Is_two_products', 'Has_Zero_Balance',
                    'Germany_Female', 'Germany_Inactive', 'Gender'] # Gender å·²ç¶“æ˜¯ 0/1

        df_copy = FeatureEngineer.cast_columns(df_copy, int_cols=int_cols, cat_cols=None) 


        # ç§»é™¤ä¸å¿…è¦çš„åŸå§‹æ¬„ä½
        cols_to_drop = ['id','CustomerId', 'Tenure','Surname', 'RowNumber' ] 
        if is_train and 'Exited' in df_copy.columns:
            cols_to_drop.append('Exited') 

        df_copy.drop(columns=[col for col in cols_to_drop if col in df_copy.columns], inplace=True, errors='ignore')
        
        # ç¢ºä¿æ‰€æœ‰æ•¸å€¼åˆ—éƒ½æ˜¯æµ®é»æ•¸
        for col in df_copy.columns:
            if df_copy[col].dtype.name not in ['object', 'category', 'str']:
                 if col not in int_cols: 
                      df_copy[col] = df_copy[col].astype(float) 

        return df_copy

    @staticmethod
    def run_v2_preprocessing(df: pd.DataFrame, is_train: bool) -> pd.DataFrame:
        """
        ç‰ˆæœ¬ 2ï¼šV1 + æ–°æ——æ¨™ is_mature_inactive_transitã€‚
        """
        original_df = df.copy() 
        
        # ä½¿ç”¨ V1 ç®¡é“ä½œç‚ºåŸºç¤
        df_copy = FeatureEngineer.run_v1_preprocessing(original_df.copy(), is_train=is_train)

        # å‰µå»ºæ–°çš„äº¤äº’ç‰¹å¾µ (ç¢ºä¿ Balance, IsActiveMember, Age å­˜åœ¨)
        if all(col in original_df.columns for col in ['Balance', 'IsActiveMember', 'Age']):
            df_copy['is_mature_inactive_transit'] = (
                                                        (original_df['Balance'] == 0) & 
                                                        (original_df['IsActiveMember'] == 0) & 
                                                        (original_df['Age'] > 40)).astype(int)
        else:
            df_copy['is_mature_inactive_transit'] = 0 # ç¼ºå¤±å‰‡è¨­ç‚º 0
        
        # ç¢ºä¿æ–°çš„æ——æ¨™æ˜¯ int
        df_copy['is_mature_inactive_transit'] = df_copy['is_mature_inactive_transit'].astype(int)
        
        # ç§»é™¤ç›®æ¨™æ¬„ä½
        if Config.TARGET_COL in df_copy.columns: 
             df_copy.drop(columns=[Config.TARGET_COL], inplace=True, errors='ignore')
        
        return df_copy
    
    # å°‡æ‰€æœ‰ FE ç®¡é“çš„åç¨±å°æ‡‰åˆ°å‡½æ•¸æœ¬èº«ï¼Œç”¨æ–¼ä¿å­˜ FE é‚è¼¯
    FE_PIPELINES: Dict[str, Callable] = {
        'run_v2_preprocessing': run_v2_preprocessing,
        'run_v1_preprocessing': run_v1_preprocessing,
    }


# --- Optuna è¶…åƒæ•¸èª¿å„ª (HyperparameterTuner) ---
class HyperparameterTuner:
    """è¶…åƒæ•¸èª¿å„ªé¡åˆ¥ï¼Œä½¿ç”¨ Optuna é€²è¡Œå„ªåŒ–ã€‚å°ˆæ³¨æ–¼ XGBoost çš„èª¿å„ªã€‚"""
    
    @staticmethod
    def _objective(trial: optuna.Trial, X: pd.DataFrame, y: pd.Series) -> float:
        """Optuna çš„ç›®æ¨™å‡½æ•¸ï¼šä½¿ç”¨äº¤å‰é©—è­‰è©•ä¼°ä¸€çµ„è¶…åƒæ•¸ã€‚"""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        }

        fixed_params = {
            'random_state': Config.RANDOM_STATE,
            'verbose': 0,
            'eval_metric': 'logloss',
            'n_jobs': -1,
            'early_stopping_rounds': 50,
            'enable_categorical': False, 
        }

        full_params = {**params, **fixed_params}
        model = XGBClassifier(**full_params)
        skf = StratifiedKFold(n_splits=Config.N_SPLITS, shuffle=True, random_state=fixed_params['random_state'])
        roc_auc_scores = []

        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]

            fit_params = {'eval_set': [(X_val, y_val)], 'verbose': False}

            try:
                model.fit(X_tr, y_tr, **fit_params)

                best_iteration = model.get_booster().best_iteration
                proba_val = model.predict_proba(X_val, iteration_range=(0, best_iteration))[:, 1]
                roc_auc_scores.append(roc_auc_score(y_val, proba_val))
            except Exception as e:
                logger.error(f"Optuna Fold {fold} è¨“ç·´éŒ¯èª¤: {e}")
                return 0.0

        return float(np.mean(roc_auc_scores))

    @staticmethod
    def tune(X: pd.DataFrame, y: pd.Series, n_trials: int) -> dict:
        """åŸ·è¡Œ Optuna èª¿å„ªä¸¦è¿”å›æœ€ä½³åƒæ•¸ã€‚"""
        optuna.logging.set_verbosity(optuna.logging.WARNING) 
        study = optuna.create_study(direction='maximize')
        objective_with_args = lambda trial: HyperparameterTuner._objective(trial, X, y)

        study.optimize(objective_with_args, n_trials=n_trials, show_progress_bar=True)

        logger.info(f"èª¿å„ªå®Œæˆã€‚æœ€ä½³ ROC AUC: {study.best_value:.5f}")
        logger.info("æœ€ä½³åƒæ•¸:")
        for key, value in study.best_params.items():
            logger.info(f" Â {key}: {value}")

        return study.best_params

# --- æ¨¡å‹è¨“ç·´å™¨é¡åˆ¥ (ModelTrainer) ---
class ModelTrainer:
    """å”èª¿å™¨é¡åˆ¥ï¼Œç”¨æ–¼çµ±ä¸€æ¨¡å‹è¨“ç·´ã€è©•ä¼°å’Œé æ¸¬çš„æµç¨‹ã€‚"""

    def __init__(self, n_splits: int = Config.N_SPLITS, random_state: int = Config.RANDOM_STATE):
        self.n_splits = n_splits
        self.random_state = random_state
        self.logger = logging.getLogger(self.__class__.__name__)

    def run_experiment(self,
                         train_df: pd.DataFrame,
                         test_df: pd.DataFrame,
                         feature_engineering_pipeline: Callable,
                         models: Dict[str, Any], 
                         target_col: str = Config.TARGET_COL) -> Tuple[pd.DataFrame, Dict[str, Any], Any, List[str]]: 
        """
        å•Ÿå‹•å®Œæ•´çš„å¯¦é©—é€±æœŸï¼šç‰¹å¾µå·¥ç¨‹ (FE)ã€è¨“ç·´ã€ç”Ÿæˆæäº¤æ–‡ä»¶ï¼Œä¸¦è¿”å›æœ€ä½³æ¨¡å‹ã€‚
        """
        self.logger.info(f"--- å•Ÿå‹•æ–°å¯¦é©— (FE: {feature_engineering_pipeline.__name__}) ---")

        test_ids = test_df['id'].copy()
        y_train = train_df[target_col].astype(int)

        # 1. ç‰¹å¾µå·¥ç¨‹
        self.logger.info("æ­¥é©Ÿ 1: æ‡‰ç”¨ç‰¹å¾µå·¥ç¨‹...")
        # è¨“ç·´é›†ï¼šä¸Ÿæ£„ç›®æ¨™åˆ—
        X_train_processed = feature_engineering_pipeline(train_df.drop(columns=[target_col], errors='ignore').copy(), is_train=True)
        # æ¸¬è©¦é›†
        X_test_processed = feature_engineering_pipeline(test_df.copy(), is_train=False)

        # ğŸ¯ è™•ç†é¡åˆ¥æ¬„ä½çš„ OHE
        
        # æ‰¾å‡ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­çš„é¡åˆ¥æ¬„ä½ (æ‡‰è©²åªæœ‰ object/str)
        cat_cols_train = [col for col in X_train_processed.columns if X_train_processed[col].dtype.name in ['object', 'str']]
        cat_cols_test = [col for col in X_test_processed.columns if X_test_processed[col].dtype.name in ['object', 'str']]
        cat_cols = list(set(cat_cols_train + cat_cols_test)) # åˆä½µä¸¦å»é‡

        # å°è¨“ç·´é›†å’Œæ¸¬è©¦é›†é€²è¡Œ One-Hot Encoding
        X_train_oh = pd.get_dummies(X_train_processed, columns=cat_cols, dummy_na=False)
        X_test_oh = pd.get_dummies(X_test_processed, columns=cat_cols, dummy_na=False)
        
        # åš´æ ¼å°é½Š (é€™æ˜¯å¿…é ˆçš„ï¼Œç¢ºä¿æ¸¬è©¦é›†å’Œè¨“ç·´é›†æœ‰ç›¸åŒçš„ OHE æ¬„ä½)
        feature_names = X_train_oh.columns.tolist()
        
        # è£œé½Šæ¸¬è©¦é›†ç¼ºå°‘çš„æ¬„ä½
        missing_cols_test = set(feature_names) - set(X_test_oh.columns)
        for c in missing_cols_test:
            X_test_oh[c] = 0
            
        # ç§»é™¤å¤šé¤˜çš„æ¬„ä½ï¼Œä¸¦ç¢ºä¿é †åºä¸€è‡´
        X_test_processed = X_test_oh[[col for col in feature_names if col in X_test_oh.columns]] # ç¢ºä¿é †åº
        X_train_processed = X_train_oh
        
        # ç¢ºä¿æ‰€æœ‰æ•¸æ“šéƒ½æ˜¯ float
        # é€™æ˜¯é—œéµæ­¥é©Ÿï¼Œç¢ºä¿æ‰€æœ‰ç‰¹å¾µ (åŒ…æ‹¬ int é¡å‹) åœ¨é€²å…¥æ¨¡å‹å‰éƒ½æ˜¯æµ®é»æ•¸
        X_train_processed = X_train_processed.astype(float)
        X_test_processed = X_test_processed.astype(float)
        
        self.logger.info(f"æœ€çµ‚ç‰¹å¾µæ•¸ (OHEå¾Œ): {len(feature_names)}")
        
        # 2. è¨“ç·´èˆ‡è©•ä¼°æ¨¡å‹
        self.logger.info("æ­¥é©Ÿ 2: åœ¨äº¤å‰é©—è­‰ä¸Šè¨“ç·´æ¨¡å‹...")
        
        models_no_cat = {}
        for name, model in models.items():
            if isinstance(model, XGBClassifier):
                # å†æ¬¡ç¢ºèªç¦ç”¨å…§å»ºé¡åˆ¥ç‰¹å¾µè™•ç†
                model.set_params(enable_categorical=False) 
            models_no_cat[name] = model

        all_results, trained_models = self._evaluate_models(models_no_cat, X_train_processed, y_train, X_test_processed)

        # 3. ç¢ºå®šæœ€ä½³æ¨¡å‹åç¨±
        self.logger.info("æ­¥é©Ÿ 3: ç¢ºå®šæ€§èƒ½æœ€ä½³çš„æ¨¡å‹åç¨±...")
        best_roc_auc = -1.0
        best_model_name = None
        best_model = None

        for name, result in all_results.items():
            if not result['metrics_df'].empty:
                current_auc = result['metrics_df']['ROC AUC'].mean()
                if current_auc > best_roc_auc:
                    best_roc_auc = current_auc
                    best_model_name = name
                    best_model = trained_models.get(name)

        if not best_model_name:
            self.logger.error("æ²’æœ‰æ¨¡å‹æˆåŠŸè¨“ç·´æˆ–è©•ä¼°ã€‚")
            return pd.DataFrame(), all_results, None, [] 

        self.logger.info(f"æœ€ä½³æ¨¡å‹: {best_model_name} (CV ROC AUC: {best_roc_auc:.4f})")

        # 4. ç”Ÿæˆæäº¤æ–‡ä»¶
        self.logger.info("æ­¥é©Ÿ 4: ç”Ÿæˆæäº¤æ–‡ä»¶...")
        submission_df = self._generate_submission(
            f"submission_{best_model_name}_{feature_engineering_pipeline.__name__}.csv",
            test_ids,
            all_results[best_model_name]['test_preds']
        )

        self.logger.info("--- å¯¦é©—æˆåŠŸå®Œæˆ ---")
        # è¿”å›è¨“ç·´æ¨¡å‹ä½¿ç”¨çš„ç‰¹å¾µé›†ï¼Œç”¨æ–¼å¾ŒçºŒ SHAP
        return submission_df, all_results, best_model, feature_names


    def _evaluate_models(self, models: Dict[str, Any], X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame) -> Tuple[Dict, Dict]: 
        """ä½¿ç”¨äº¤å‰é©—è­‰è¨“ç·´å’Œé©—è­‰æ¨¡å‹ï¼Œä¸¦è¿”å›æ¯å€‹æ¨¡å‹çš„æœ€çµ‚è¨“ç·´å¯¦ä¾‹ã€‚"""
        self.logger.info("å•Ÿå‹•äº¤å‰é©—è­‰...")
        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)
        results = {}
        trained_models = {}

        for name, model in models.items():
            self.logger.info(f"æ­£åœ¨è¨“ç·´æ¨¡å‹: {name}")
            oof_preds = np.zeros(len(X_train))
            test_preds_folds, fold_metrics_list = [], []
            final_model_instance = None # ä¿å­˜æœ€å¾Œä¸€å€‹æŠ˜ç–Šè¨“ç·´çš„æ¨¡å‹å¯¦ä¾‹

            # é€²è¡Œ K æŠ˜äº¤å‰é©—è­‰
            for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

                current_model = clone(model)
                fit_params = {}

                try:
                    # --- XGBoost ç‰¹å®šé‚è¼¯ ---
                    if isinstance(current_model, XGBClassifier):
                        fit_params['eval_set'] = [(X_val, y_val)]
                        fit_params['verbose'] = False # è¨­ç½® XGBoost éœé»˜æ¨¡å¼
                        
                        current_model.fit(X_tr, y_tr, **fit_params)

                        best_iteration = current_model.get_booster().best_iteration
                        proba_val = current_model.predict_proba(X_val, iteration_range=(0, best_iteration))[:, 1]
                        proba_test = current_model.predict_proba(X_test, iteration_range=(0, best_iteration))[:, 1]
                    else:
                        current_model.fit(X_tr, y_tr)
                        proba_val = current_model.predict_proba(X_val)[:, 1]
                        proba_test = current_model.predict_proba(X_test)[:, 1]
                    # -------------------------

                    oof_preds[val_idx] = proba_val
                    test_preds_folds.append(proba_test)

                    # æ”¶é›†æŒ‡æ¨™
                    fold_metrics_list.append(
                        {'ROC AUC': roc_auc_score(y_val, proba_val)}
                    ) 
                    
                    final_model_instance = current_model

                except Exception as e:
                    self.logger.error(f"æ¨¡å‹ {name} åœ¨æŠ˜ç–Š {fold} è¨“ç·´æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue

            # å„²å­˜çµæœ
            results[name] = {
                'oof_preds': oof_preds,
                'test_preds': np.mean(test_preds_folds, axis=0) if test_preds_folds else np.zeros(len(X_test)),
                'metrics_df': pd.DataFrame(fold_metrics_list),
            }
            if final_model_instance:
                trained_models[name] = final_model_instance 
                
            if not results[name]['metrics_df'].empty:
                self.logger.info(
                    f" æ¨¡å‹ {name} | CV ROC AUC: {results[name]['metrics_df']['ROC AUC'].mean():.4f} Â± {results[name]['metrics_df']['ROC AUC'].std():.4f}")
            else:
                 self.logger.warning(f"æ¨¡å‹ {name} è¨“ç·´å¤±æ•—ï¼Œç„¡æ³•è¨ˆç®— CV ROC AUCã€‚")

        return results, trained_models

    def _generate_submission(self, filename: str, df_test_id: pd.Series, test_preds: np.ndarray) -> pd.DataFrame:
        """ç”Ÿæˆæäº¤æ–‡ä»¶ã€‚"""
        # ç°¡åŒ–æäº¤æ–‡ä»¶å
        if 'submission_XGBoost_Final_Tuned_run_v2_preprocessing' in filename:
             filename = 'submission.csv' 
        
        submission_df = pd.DataFrame({'id': df_test_id, 'Exited': test_preds})
        submission_df.to_csv(filename, index=False)
        self.logger.info(f"æäº¤æ–‡ä»¶æˆåŠŸä¿å­˜: {filename}")
        return submission_df

    def save_model_and_params(self, 
                              model: Any, 
                              fe_pipeline_name: str, 
                              model_name: str, 
                              best_params: Dict[str, Any], 
                              output_path: str = Config.MODEL_DIR) -> None:
        """ä¿å­˜æ¨¡å‹ã€ç‰¹å¾µå·¥ç¨‹ç®¡é“åç¨±å’Œæœ€ä½³åƒæ•¸ã€‚"""
        model_filename = "churn_bank_model.joblib" 
        
        # 1. ä¿å­˜æ¨¡å‹
        full_model_path = os.path.join(output_path, model_filename)
        try:
            joblib.dump(model, full_model_path)
            self.logger.info(f"æ¨¡å‹æˆåŠŸä¿å­˜è‡³: {full_model_path}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # 2. ä¿å­˜ç‰¹å¾µå·¥ç¨‹ç®¡é“åç¨±
        fe_pipeline_name_path = os.path.join(output_path, 'fe_pipeline_name.txt')
        try:
            with open(fe_pipeline_name_path, 'w') as f:
                f.write(fe_pipeline_name)
            self.logger.info(f"ç‰¹å¾µå·¥ç¨‹ç®¡é“åç¨± '{fe_pipeline_name}' æˆåŠŸä¿å­˜è‡³: {fe_pipeline_name_path}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ FE ç®¡é“åç¨±æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            

# --- ä¸»åŸ·è¡Œå‡½æ•¸ ---
def main(train_file: str, test_file: str, tune: bool, n_trials: int):
    
    logger.info(f"é–‹å§‹åŸ·è¡Œè…³æœ¬ã€‚è¨“ç·´æ–‡ä»¶: {train_file}, æ¸¬è©¦æ–‡ä»¶: {test_file}")
    
    # æ•¸æ“šåŠ è¼‰
    try:
        df_train = pd.read_csv(train_file)
        
        # ğŸ¯ æ–¹æ¡ˆ A ä¿®æ­£ï¼šå‡è¨­ test.csv åŒ…å«æ¨™é ­ (Header)
        df_test = pd.read_csv(
            test_file, 
            header=0, # å‡è¨­ test.csv åŒ…å«æ¨™é ­è¡Œï¼Œä½¿ç”¨ç¬¬ 0 è¡Œä½œç‚ºæ¬„ä½åç¨±
            # ç§»é™¤ names åƒæ•¸ï¼Œè®“ pandas è‡ªå‹•ä½¿ç”¨æ¨™é ­
            # è¼”åŠ©ï¼šå˜—è©¦åœ¨è®€å–æ™‚å°±å°‡æ•¸å€¼æ¬„ä½è®€å–ç‚º float
            dtype={'CreditScore': float, 'Age': float, 'Tenure': float, 
                   'Balance': float, 'NumOfProducts': float, 'HasCrCard': float, 
                   'IsActiveMember': float, 'EstimatedSalary': float}
        ) 
        
        logger.info(f"è¨“ç·´æ•¸æ“šå¤§å°: {df_train.shape}, æ¸¬è©¦æ•¸æ“šå¤§å°: {df_test.shape}")
        
    except FileNotFoundError:
        logger.error("éŒ¯èª¤ï¼šè«‹ç¢ºä¿è¨“ç·´å’Œæ¸¬è©¦æ–‡ä»¶å­˜åœ¨æ–¼æŒ‡å®šè·¯å¾‘ã€‚")
        return
    except Exception as e:
        logger.error(f"æ•¸æ“šåŠ è¼‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        # å¦‚æœä¿®æ­£å¾Œä»ç„¶å ±éŒ¯ï¼Œå°‡é¡å¤–æ‰“å° DataFrame çš„å‰å¹¾è¡Œè³‡è¨Šä»¥ä¾›é€²ä¸€æ­¥èª¿è©¦
        # if 'df_train' in locals() and 'df_test' in locals():
        #     logger.error(f"df_train columns: {df_train.columns.tolist()}")
        #     logger.error(f"df_test columns: {df_test.columns.tolist()}")
        return

    trainer = ModelTrainer()
    
    # é¸æ“‡æœ€ä½³ç‰¹å¾µå·¥ç¨‹ç®¡é“
    best_fe_pipeline = FeatureEngineer.run_v2_preprocessing
    FE_PIPELINE_NAME = best_fe_pipeline.__name__
    MODEL_NAME = 'XGBoost_Final_Tuned'

    # --- è¶…åƒæ•¸èª¿å„ªï¼ˆå¯é¸ï¼‰---
    if tune:
        logger.info("--- å•Ÿå‹• Optuna è¶…åƒæ•¸èª¿å„ªæ¨¡å¼ ---")
        
        # è‡¨æ™‚è™•ç†æ•¸æ“šä»¥é€²è¡Œèª¿å„ª
        X_train_temp = best_fe_pipeline(df_train.drop(columns=[Config.TARGET_COL], errors='ignore').copy(), is_train=True)
        y_train_temp = df_train[Config.TARGET_COL].astype(int)
        
        # OHE æ•¸æ“šä»¥é€²è¡Œèª¿å„ª
        cat_cols = [col for col in X_train_temp.columns if X_train_temp[col].dtype.name in ['object', 'str']]
        X_train_oh = pd.get_dummies(X_train_temp, columns=cat_cols, dummy_na=False)
        X_train_temp = X_train_oh.astype(float) # ç¢ºä¿æ˜¯æµ®é»æ•¸
        
        final_best_params = HyperparameterTuner.tune(X_train_temp, y_train_temp, n_trials)
        
        # è¨­ç½® Optuna åƒæ•¸ç‚ºæœ€çµ‚æ¨¡å‹åƒæ•¸
        final_best_params['random_state'] = Config.RANDOM_STATE
        final_best_params['eval_metric'] = 'logloss'
        final_best_params['n_jobs'] = -1
        final_best_params['early_stopping_rounds'] = final_best_params.get('early_stopping_rounds', 50)
        final_best_params['enable_categorical'] = False 
        final_best_params['verbose'] = 0

    else:
        # ä½¿ç”¨ç¡¬ç·¨ç¢¼çš„æœ€ä½³åƒæ•¸
        logger.info("--- ä½¿ç”¨ç¡¬ç·¨ç¢¼çš„æœ€ä½³åƒæ•¸ ---")
        final_best_params = {
            'n_estimators': 2692,
            'learning_rate': 0.05786197845936901,
            'max_depth': 3,
            'reg_lambda': 1.0628185137032307e-08,
            'reg_alpha': 3.255737505871401,
            'subsample': 0.8409191153520594,
            'colsample_bytree': 0.7834673458794292,
            # å›ºå®šçš„åƒæ•¸
            'random_state': Config.RANDOM_STATE,
            'eval_metric': 'logloss',
            'n_jobs': -1,
            'early_stopping_rounds': 50,
            'enable_categorical': False, 
            'verbose': 0
        }

    # å¯¦ä¾‹åŒ–æœ€çµ‚æ¨¡å‹
    final_tuned_model = XGBClassifier(**final_best_params)
    models_final = {MODEL_NAME: final_tuned_model}

    # é‹è¡Œæœ€çµ‚å¯¦é©— (run_experiment å…§éƒ¨æœƒé€²è¡Œ OHE ä¸¦è½‰æ›ç‚º float)
    submission_final, results_final, best_model_cv, feature_cols = trainer.run_experiment(
        train_df=df_train,
        test_df=df_test,
        feature_engineering_pipeline=best_fe_pipeline,
        models=models_final
    )
    
    if submission_final.empty or not best_model_cv:
        logger.error("å¯¦é©—å¤±æ•—ï¼Œç„¡æ³•ç”Ÿæˆæäº¤æ–‡ä»¶æˆ–ç²å–è¨“ç·´æ¨¡å‹ã€‚è…³æœ¬çµ‚æ­¢ã€‚")
        return

    # --- æ­¥é©Ÿ 5: ä¿å­˜æ¨¡å‹ã€ç‰¹å¾µå·¥ç¨‹ç®¡é“åç¨±å’Œç‰¹å¾µåˆ—è¡¨ --- 
    trainer.save_model_and_params(
        model=best_model_cv, 
        fe_pipeline_name=FE_PIPELINE_NAME, 
        model_name=MODEL_NAME, 
        best_params=final_best_params
    )
    
    # é¡å¤–ä¿å­˜ç‰¹å¾µæ¬„ä½åˆ—è¡¨
    feature_list_path = os.path.join(Config.MODEL_DIR, 'feature_columns.joblib')
    joblib.dump(feature_cols, feature_list_path) 
    logger.info(f"ç‰¹å¾µæ¬„ä½åˆ—è¡¨æˆåŠŸä¿å­˜è‡³: {feature_list_path}")


# --- è…³æœ¬å…¥å£é» ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="éŠ€è¡Œå®¢æˆ¶æµå¤±é æ¸¬ - XGBoost Optuna/SHAP æ•´åˆç‰ˆè¨“ç·´è…³æœ¬")
    
    # é è¨­è·¯å¾‘ (è«‹æ ¹æ“šå¯¦éš›æƒ…æ³ä¿®æ”¹)
    default_root = os.path.dirname(os.path.abspath(__file__))
    default_train_path = os.path.join(default_root, "train.csv") 
    default_test_path = os.path.join(default_root, "test.csv")

    parser.add_argument("--train_file", type=str, default=default_train_path, help="è¨“ç·´æ•¸æ“šæ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--test_file", type=str, default=default_test_path, help="æ¸¬è©¦æ•¸æ“šæ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--tune", action="store_true", help="æ˜¯å¦åŸ·è¡Œ Optuna è¶…åƒæ•¸èª¿å„ª")
    parser.add_argument("--n_trials", type=int, default=50, help="Optuna èª¿å„ªçš„è¿­ä»£æ¬¡æ•¸")
    
    args = parser.parse_args()
    
    # åŸ·è¡Œä¸»å‡½æ•¸
    main(args.train_file, args.test_file, args.tune, args.n_trials)