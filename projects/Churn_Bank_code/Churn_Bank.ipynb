{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33780,
     "status": "ok",
     "timestamp": 1764223245168,
     "user": {
      "displayName": "中正大學激動哥",
      "userId": "04626570378575691125"
     },
     "user_tz": -480
    },
    "id": "e0uWVieA4rpj",
    "outputId": "5ba7ec50-62b6-4dd0-a55d-2df84223f2c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna) (25.0)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.44-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting tqdm (from optuna)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting PyYAML (from optuna)\n",
      "  Downloading pyyaml-6.0.3-cp311-cp311-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "Downloading alembic-1.17.2-py3-none-any.whl (248 kB)\n",
      "Downloading sqlalchemy-2.0.44-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 30.1 MB/s  0:00:00\n",
      "Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading pyyaml-6.0.3-cp311-cp311-win_amd64.whl (158 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, PyYAML, Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "\n",
      "   ---------- ----------------------------- 2/8 [Mako]\n",
      "   ------------------------- -------------- 5/8 [sqlalchemy]\n",
      "   ------------------------- -------------- 5/8 [sqlalchemy]\n",
      "   ------------------------- -------------- 5/8 [sqlalchemy]\n",
      "   ------------------------- -------------- 5/8 [sqlalchemy]\n",
      "   ------------------------- -------------- 5/8 [sqlalchemy]\n",
      "   ------------------------- -------------- 5/8 [sqlalchemy]\n",
      "   ------------------------- -------------- 5/8 [sqlalchemy]\n",
      "   ------------------------------ --------- 6/8 [alembic]\n",
      "   ----------------------------------- ---- 7/8 [optuna]\n",
      "   ----------------------------------- ---- 7/8 [optuna]\n",
      "   ----------------------------------- ---- 7/8 [optuna]\n",
      "   ---------------------------------------- 8/8 [optuna]\n",
      "\n",
      "Successfully installed Mako-1.3.10 PyYAML-6.0.3 alembic-1.17.2 colorlog-6.10.1 greenlet-3.2.4 optuna-4.6.0 sqlalchemy-2.0.44 tqdm-4.67.1\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from xgboost) (2.3.5)\n",
      "Collecting scipy (from xgboost)\n",
      "  Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Downloading xgboost-3.1.2-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 3.4/72.0 MB 25.2 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 15.5/72.0 MB 40.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 26.7/72.0 MB 45.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 38.0/72.0 MB 48.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 49.5/72.0 MB 50.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 61.3/72.0 MB 51.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 52.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 46.8 MB/s  0:00:01\n",
      "Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "   ---------------------------------------- 0.0/38.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 12.6/38.7 MB 60.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.6/38.7 MB 59.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.8/38.7 MB 55.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.7 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.7/38.7 MB 39.7 MB/s  0:00:00\n",
      "Installing collected packages: scipy, xgboost\n",
      "\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   ---------------------------------------- 0/2 [scipy]\n",
      "   -------------------- ------------------- 1/2 [xgboost]\n",
      "   -------------------- ------------------- 1/2 [xgboost]\n",
      "   -------------------- ------------------- 1/2 [xgboost]\n",
      "   -------------------- ------------------- 1/2 [xgboost]\n",
      "   ---------------------------------------- 2/2 [xgboost]\n",
      "\n",
      "Successfully installed scipy-1.16.3 xgboost-3.1.2\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from plotly) (25.0)\n",
      "Downloading plotly-6.5.0-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 3.7/9.9 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 32.4 MB/s  0:00:00\n",
      "Downloading narwhals-2.12.0-py3-none-any.whl (425 kB)\n",
      "Installing collected packages: narwhals, plotly\n",
      "\n",
      "   ---------------------------------------- 0/2 [narwhals]\n",
      "   ---------------------------------------- 0/2 [narwhals]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   -------------------- ------------------- 1/2 [plotly]\n",
      "   ---------------------------------------- 2/2 [plotly]\n",
      "\n",
      "Successfully installed narwhals-2.12.0 plotly-6.5.0\n",
      "Collecting optuna-integration[xgboost]\n",
      "  Downloading optuna_integration-4.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: optuna in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna-integration[xgboost]) (4.6.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna-integration[xgboost]) (3.1.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (1.17.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (6.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (2.0.44)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from optuna->optuna-integration[xgboost]) (6.0.3)\n",
      "Requirement already satisfied: Mako in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from alembic>=1.5.0->optuna->optuna-integration[xgboost]) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from alembic>=1.5.0->optuna->optuna-integration[xgboost]) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[xgboost]) (3.2.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from colorlog->optuna->optuna-integration[xgboost]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration[xgboost]) (3.0.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\envs\\ai_web\\lib\\site-packages (from xgboost->optuna-integration[xgboost]) (1.16.3)\n",
      "Downloading optuna_integration-4.6.0-py3-none-any.whl (99 kB)\n",
      "Installing collected packages: optuna-integration\n",
      "Successfully installed optuna-integration-4.6.0\n"
     ]
    }
   ],
   "source": [
    "# 確保環境安裝必要的庫\n",
    "# 請在 Jupyter/Colab 單元格中執行以下指令\n",
    "\n",
    "%pip install optuna\n",
    "%pip install xgboost==1.7.6  # 關鍵：鎖定版本以確保與訓練邏輯兼容\n",
    "%pip install plotly\n",
    "%pip install optuna-integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7984,
     "status": "ok",
     "timestamp": 1764223262168,
     "user": {
      "displayName": "中正大學激動哥",
      "userId": "04626570378575691125"
     },
     "user_tz": -480
    },
    "id": "PV2h86bc4tp5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 確保在 Notebook 中顯示圖表\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import Any, Callable, Tuple, Dict, List\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.base import clone\n",
    "import optuna\n",
    "# 導入但不使用，因為它需要新的 callbacks 參數\n",
    "from optuna.integration import XGBoostPruningCallback \n",
    "\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1764223265146,
     "user": {
      "displayName": "中正大學激動哥",
      "userId": "04626570378575691125"
     },
     "user_tz": -480
    },
    "id": "yFRruAlk45vu"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    TARGET_COL = 'Exited'\n",
    "    N_SPLITS = 5\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "# --- 數據加載（模擬）---\n",
    "# 由於無法訪問您的本地文件，我們創建一個模擬數據集以確保邏輯可執行。\n",
    "# 在實際執行時，請將下面的模擬代碼替換為真實的數據加載。\n",
    "\n",
    "try:\n",
    "    # 嘗試加載真實文件\n",
    "    df_train = pd.read_csv(\"train.csv\")\n",
    "    df_test = pd.read_csv(\"test.csv\")\n",
    "    print(f\"訓練數據大小: {df_train.shape}, 測試數據大小: {df_test.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ 警告：未找到 'train.csv' 和 'test.csv'。正在創建模擬數據。\")\n",
    "\n",
    "    # 模擬訓練數據\n",
    "    np.random.seed(Config.RANDOM_STATE)\n",
    "    n_train = 165000\n",
    "    df_train = pd.DataFrame({\n",
    "        'RowNumber': range(1, n_train + 1),\n",
    "        'CustomerId': range(10000000, 10000000 + n_train),\n",
    "        'Surname': ['Surname' + str(i) for i in range(n_train)],\n",
    "        'CreditScore': np.random.randint(300, 850, n_train),\n",
    "        'Geography': np.random.choice(['France', 'Spain', 'Germany'], n_train, p=[0.5, 0.25, 0.25]),\n",
    "        'Gender': np.random.choice(['Male', 'Female'], n_train),\n",
    "        'Age': np.random.randint(18, 90, n_train),\n",
    "        'Tenure': np.random.randint(0, 10, n_train),\n",
    "        'Balance': np.abs(np.random.normal(loc=50000, scale=60000, size=n_train)),\n",
    "        'NumOfProducts': np.random.randint(1, 5, n_train),\n",
    "        'HasCrCard': np.random.randint(0, 2, n_train),\n",
    "        'IsActiveMember': np.random.randint(0, 2, n_train),\n",
    "        'EstimatedSalary': np.random.uniform(1000, 200000, n_train),\n",
    "        'Exited': np.random.randint(0, 2, n_train, p=[0.8, 0.2])\n",
    "    })\n",
    "    # 模擬測試數據\n",
    "    n_test = 110000\n",
    "    df_test = pd.DataFrame({\n",
    "        'id': range(n_test),\n",
    "        'RowNumber': range(n_train + 1, n_train + n_test + 1),\n",
    "        'CustomerId': range(10000000 + n_train, 10000000 + n_train + n_test),\n",
    "        'Surname': ['Surname' + str(i) for i in range(n_train, n_train + n_test)],\n",
    "        'CreditScore': np.random.randint(300, 850, n_test),\n",
    "        'Geography': np.random.choice(['France', 'Spain', 'Germany'], n_test, p=[0.5, 0.25, 0.25]),\n",
    "        'Gender': np.random.choice(['Male', 'Female'], n_test),\n",
    "        'Age': np.random.randint(18, 90, n_test),\n",
    "        'Tenure': np.random.randint(0, 10, n_test),\n",
    "        'Balance': np.abs(np.random.normal(loc=50000, scale=60000, size=n_test)),\n",
    "        'NumOfProducts': np.random.randint(1, 5, n_test),\n",
    "        'HasCrCard': np.random.randint(0, 2, n_test),\n",
    "        'IsActiveMember': np.random.randint(0, 2, n_test),\n",
    "        'EstimatedSalary': np.random.uniform(1000, 200000, n_test),\n",
    "    })\n",
    "    \n",
    "    # 調整模擬數據以匹配某些特徵工程假設\n",
    "    df_train.loc[df_train['Balance'] < 1, 'Balance'] = 0\n",
    "    df_test.loc[df_test['Balance'] < 1, 'Balance'] = 0\n",
    "\n",
    "    print(f\"訓練數據大小 (模擬): {df_train.shape}, 測試數據大小 (模擬): {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1764223267819,
     "user": {
      "displayName": "中正大學激動哥",
      "userId": "04626570378575691125"
     },
     "user_tz": -480
    },
    "id": "UaOg7sQe5DDv"
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    用於特徵工程的工具類別。\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def map_columns(df: pd.DataFrame, mappings: dict) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        for col, mapping in mappings.items():\n",
    "            if col in df_copy.columns:\n",
    "                df_copy[col] = df_copy[col].map(mapping)\n",
    "        return df_copy\n",
    "\n",
    "    @staticmethod\n",
    "    def cast_columns(df: pd.DataFrame, int_cols: Any = None,\n",
    "                     cat_cols: Any = None) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        if int_cols:\n",
    "            for col in int_cols:\n",
    "                if col in df_copy.columns:\n",
    "                    df_copy[col] = df_copy[col].astype(int)\n",
    "        if cat_cols:\n",
    "            for col in cat_cols:\n",
    "                if col in df_copy.columns:\n",
    "                    df_copy[col] = df_copy[col].astype('category')\n",
    "        return df_copy\n",
    "\n",
    "    @staticmethod\n",
    "    def run_v0_baseline(df: pd.DataFrame, is_train: bool) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        int_cols = ['HasCrCard', 'IsActiveMember']\n",
    "        cat_cols = ['Geography', 'Gender']\n",
    "        df_copy = FeatureEngineer.cast_columns(df_copy, int_cols=int_cols, cat_cols=cat_cols)\n",
    "\n",
    "        cols_to_drop = ['CustomerId','Surname']\n",
    "        if is_train and 'Exited' in df_copy.columns:\n",
    "             cols_to_drop.append('Exited')\n",
    "        df_copy.drop(columns=[col for col in cols_to_drop if col in df_copy.columns], inplace=True, errors='ignore')\n",
    "        return df_copy\n",
    "\n",
    "    @staticmethod\n",
    "    def run_v1_preprocessing(df: pd.DataFrame, is_train: bool) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        gender_map = {'Male': 0, 'Female': 1}\n",
    "        df_copy = FeatureEngineer.map_columns(df_copy, {'Gender': gender_map})\n",
    "\n",
    "        # 年齡分箱\n",
    "        df_copy['Age_bin'] = pd.cut(df_copy['Age'], bins=[0, 25, 35, 45, 60, np.inf],\n",
    "                                     labels=['very_young', 'young', 'mid', 'mature', 'senior'])\n",
    "\n",
    "        # 創建基礎特徵旗標\n",
    "        df_copy['Is_two_products'] = (df_copy['NumOfProducts'] == 2).astype(int)\n",
    "        df_copy['Germany_Female'] = ((df_copy['Geography'] == 'Germany') & (df_copy['Gender'] == 1)).astype(int)\n",
    "        df_copy['Germany_Inactive'] = ((df_copy['Geography'] == 'Germany') & (df_copy['IsActiveMember'] == 0)).astype(int)\n",
    "        df_copy['Has_Zero_Balance'] = (df_copy['Balance'] == 0).astype(int)\n",
    "\n",
    "        # 對 Tenure 進行 Log 轉換\n",
    "        df_copy['Tenure_log'] = np.log1p(df_copy['Tenure'])\n",
    "\n",
    "        int_cols = ['HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Is_two_products', 'Has_Zero_Balance',\n",
    "                    'Germany_Female', 'Germany_Inactive']\n",
    "        cat_cols = ['Geography', 'Age_bin']\n",
    "\n",
    "        df_copy = FeatureEngineer.cast_columns(df_copy, int_cols=int_cols, cat_cols=cat_cols)\n",
    "\n",
    "        cols_to_drop = ['CustomerId', 'Tenure','Surname', 'RowNumber' ] # 新增 RowNumber\n",
    "        if is_train and 'Exited' in df_copy.columns:\n",
    "            cols_to_drop.append('Exited')\n",
    "\n",
    "        df_copy.drop(columns=[col for col in cols_to_drop if col in df_copy.columns], inplace=True, errors='ignore')\n",
    "        return df_copy\n",
    "\n",
    "    @staticmethod\n",
    "    def run_v2_preprocessing(df: pd.DataFrame, is_train: bool) -> pd.DataFrame:\n",
    "        \"\"\"版本 2：V1 + 新旗標 is_mature_inactive_transit。\"\"\"\n",
    "        # 注意：這裡 run_v1_preprocessing 內已處理 'Gender' 映射\n",
    "        df_copy = FeatureEngineer.run_v1_preprocessing(df, is_train=False)\n",
    "\n",
    "        # 創建新的交互特徵\n",
    "        df_copy['is_mature_inactive_transit'] = (\n",
    "                    (df_copy['Has_Zero_Balance'] == 1) & (df_copy['IsActiveMember'] == 0) & (\n",
    "                    df_copy['Age'] > 40)).astype(int)\n",
    "\n",
    "        # 刪除 Exited 欄位 (如果在 V1 預處理中未被刪除，則再次嘗試刪除，但 V1 已處理)\n",
    "        # 這裡的邏輯是確保最終特徵集不包含 Target\n",
    "        if is_train and 'Exited' in df.columns: # 應檢查原始 df\n",
    "            df_copy.drop(columns=['Exited'], inplace=True, errors='ignore')\n",
    "\n",
    "        return df_copy\n",
    "\n",
    "    @staticmethod\n",
    "    def run_v3_preprocessing(df: pd.DataFrame, is_train: bool) -> pd.DataFrame:\n",
    "        \"\"\"版本 3：V1 + 多項式/交互特徵。\"\"\"\n",
    "        df_copy = FeatureEngineer.run_v1_preprocessing(df, is_train=False)\n",
    "\n",
    "        # 創建交互特徵\n",
    "        df_copy['Balance_per_product'] = df_copy['Balance'] / (df_copy['NumOfProducts'] + 1e-9)\n",
    "        df_copy['Age_x_Tenure'] = df_copy['Age'] * df_copy['Tenure_log']\n",
    "        df_copy['CreditScore_x_Age'] = df_copy['CreditScore'] * df_copy['Age']\n",
    "\n",
    "        # 刪除 Exited 欄位 (確保最終特徵集不包含 Target)\n",
    "        if is_train and 'Exited' in df.columns: # 應檢查原始 df\n",
    "            df_copy.drop(columns=['Exited'], inplace=True, errors='ignore')\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1764223270526,
     "user": {
      "displayName": "中正大學激動哥",
      "userId": "04626570378575691125"
     },
     "user_tz": -480
    },
    "id": "hbiU6lgB5GCG"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('ModelTrainer')\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1764223271432,
     "user": {
      "displayName": "中正大學激動哥",
      "userId": "04626570378575691125"
     },
     "user_tz": -480
    },
    "id": "eOJ3Gw5J5Of-"
   },
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    \"\"\"\n",
    "    超參數調優類別，使用 Optuna 進行優化。\n",
    "    專注於 XGBoost 的調優。\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _objective(trial: optuna.Trial, X: pd.DataFrame, y: pd.Series, cat_feature_names: list) -> float:\n",
    "        \"\"\"\n",
    "        Optuna 的目標函數：使用交叉驗證評估一組超參數。\n",
    "        \"\"\"\n",
    "        model_name = 'XGBoost' # 假設我們只調優 XGBoost\n",
    "\n",
    "        # 1. 定義要調優的 XGBoost 參數空間\n",
    "        params = {\n",
    "            # 樹參數\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            # 正則化參數\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            # 隨機參數\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        }\n",
    "\n",
    "        # 2. 定義固定參數\n",
    "        fixed_params = {\n",
    "            'random_state': 42,\n",
    "            'verbose': 0,\n",
    "            'eval_metric': 'logloss',\n",
    "            'n_jobs': -1,\n",
    "            'early_stopping_rounds': 50,\n",
    "            'enable_categorical': True, # 啟用原生類別特徵支持\n",
    "        }\n",
    "\n",
    "        full_params = {**params, **fixed_params}\n",
    "\n",
    "        # 3. 創建模型\n",
    "        model = XGBClassifier(**full_params)\n",
    "\n",
    "        # 4. 交叉驗證與擬合\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=fixed_params['random_state'])\n",
    "        roc_auc_scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            # --- 關鍵修正區塊 ---\n",
    "            # 確保只傳遞 XGBoost 接受的參數\n",
    "            fit_params = {\n",
    "                'eval_set': [(X_val, y_val)],\n",
    "                'verbose': False\n",
    "            }\n",
    "            # 'early_stopping_rounds' 已經在 full_params 中，會自動傳遞給 fit\n",
    "            # 我們不能傳遞 'callbacks'\n",
    "            # --------------------\n",
    "\n",
    "            model.fit(X_tr, y_tr, **fit_params)\n",
    "\n",
    "            # 確保我們使用訓練完成的模型進行預測\n",
    "            best_iteration = model.get_booster().best_iteration\n",
    "\n",
    "            # 使用最佳迭代次數預測\n",
    "            proba_val = model.predict_proba(X_val, iteration_range=(0, best_iteration))[:, 1]\n",
    "            roc_auc_scores.append(roc_auc_score(y_val, proba_val))\n",
    "\n",
    "        # 5. 返回平均 ROC AUC 分數（Optuna 將嘗試最大化此值）\n",
    "        return np.mean(roc_auc_scores)\n",
    "\n",
    "    @staticmethod\n",
    "    def tune(X: pd.DataFrame, y: pd.Series, cat_feature_names: list, n_trials: int) -> dict:\n",
    "        \"\"\"\n",
    "        執行 Optuna 調優並返回最佳參數。\n",
    "        \"\"\"\n",
    "        # 創建 Optuna 研究 (Study)\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "\n",
    "        # 包裝目標函數，傳遞數據\n",
    "        objective_with_args = lambda trial: HyperparameterTuner._objective(trial, X, y, cat_feature_names)\n",
    "\n",
    "        # 開始優化\n",
    "        study.optimize(objective_with_args, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "        print(f\"調優完成。最佳 ROC AUC: {study.best_value:.5f}\")\n",
    "        print(\"最佳參數:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        # 返回最佳參數\n",
    "        return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1764223273730,
     "user": {
      "displayName": "中正大學激動哥",
      "userId": "04626570378575691125"
     },
     "user_tz": -480
    },
    "id": "YSi--15b5KBX"
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"協調器類別，用於統一模型訓練、評估和預測的流程。\"\"\"\n",
    "\n",
    "    def __init__(self, n_splits: int = Config.N_SPLITS, random_state: int = Config.RANDOM_STATE):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "        if not self.logger.handlers:\n",
    "            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    def run_experiment_tune(self,\n",
    "                       train_df: pd.DataFrame,\n",
    "                       test_df: pd.DataFrame,\n",
    "                       feature_engineering_pipeline: Callable,\n",
    "                       models: dict, # 使用 dict\n",
    "                       target_col: str = Config.TARGET_COL,\n",
    "                       tune_hyperparams: bool = False,\n",
    "                       tune_model_name: str = 'XGBoost', # 預設為 XGBoost\n",
    "                       n_trials: int = 50) -> tuple[pd.DataFrame, dict, pd.DataFrame, plt.Figure]: # 使用 tuple 和 dict\n",
    "        \"\"\"\n",
    "        啟動完整的實驗週期，可選配超參數調優。\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"--- 啟動新實驗 (特徵工程 FE: {feature_engineering_pipeline.__name__}) ---\")\n",
    "        if tune_hyperparams:\n",
    "            self.logger.info(f\"!!! 已為模型 '{tune_model_name}' 啟用超參數調優模式 !!!\")\n",
    "\n",
    "        test_ids = test_df['id'].copy()\n",
    "        original_train_for_analysis = train_df.copy()\n",
    "        y_train = train_df[target_col].astype(int)\n",
    "\n",
    "\n",
    "        self.logger.info(\"步驟 1: 應用特徵工程...\")\n",
    "        X_train_processed = feature_engineering_pipeline(train_df, is_train=True)\n",
    "        X_test_processed = feature_engineering_pipeline(test_df, is_train=False)\n",
    "\n",
    "\n",
    "        train_cols = X_train_processed.columns\n",
    "        test_cols = X_test_processed.columns\n",
    "        if not train_cols.equals(test_cols):\n",
    "            self.logger.warning(\"訓練集和測試集的欄位不一致! 正在對齊...\")\n",
    "            shared_cols = list(train_cols.intersection(test_cols))\n",
    "            X_train_processed = X_train_processed[shared_cols]\n",
    "            X_test_processed = X_test_processed[shared_cols]\n",
    "\n",
    "\n",
    "        models_to_train = models.copy()\n",
    "\n",
    "        if tune_hyperparams:\n",
    "            if tune_model_name not in models:\n",
    "                self.logger.error(\n",
    "                    f\"用於調優的模型 '{tune_model_name}' 未在 models 字典中找到。調優已取消。\")\n",
    "            else:\n",
    "                self.logger.info(f\"步驟 1.5: 為 '{tune_model_name}' 進行超參數調優...\")\n",
    "\n",
    "                cat_features = X_train_processed.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "\n",
    "                best_params = HyperparameterTuner.tune(\n",
    "                    X=X_train_processed,\n",
    "                    y=y_train,\n",
    "                    cat_feature_names=cat_features,\n",
    "                    n_trials=n_trials\n",
    "                )\n",
    "\n",
    "                # 確保 XGBoost 需要的固定參數被包含\n",
    "                best_params['random_state'] = self.random_state\n",
    "                best_params['verbose'] = 0\n",
    "                best_params['eval_metric'] = 'logloss'\n",
    "                best_params['n_jobs'] = -1\n",
    "                best_params['verbosity'] = 0\n",
    "\n",
    "                # 處理早停參數 (若 Optuna 未調優此參數，則使用預設值)\n",
    "                if 'early_stopping_rounds' not in best_params:\n",
    "                    best_params['early_stopping_rounds'] = 50\n",
    "\n",
    "                # 關鍵修正：實例化 XGBClassifier\n",
    "                # 由於我們現在只專注於 XGBoost，這裡假設 HyperparameterTuner.tune 返回的是 XGBoost 參數\n",
    "                tuned_model = XGBClassifier(**best_params)\n",
    "\n",
    "\n",
    "                tuned_model_name = f\"{tune_model_name}_Tuned\"\n",
    "                models_to_train = {tuned_model_name: tuned_model}\n",
    "                self.logger.info(f\"調優完成。模型 '{tuned_model_name}' 將用於訓練。\")\n",
    "\n",
    "        # 2. 訓練與評估模型\n",
    "        self.logger.info(\"步驟 2: 在交叉驗證上訓練模型...\")\n",
    "        all_results = self._evaluate_models(models_to_train, X_train_processed, y_train, X_test_processed)\n",
    "\n",
    "        # 3. 錯誤分析(暫時不用，因此需要修正)\n",
    "        # self.logger.info(\"步驟 3: 分析最佳模型的錯誤...\")\n",
    "        # best_model_name, error_df, dashboard_figure = ErrorAnalyzer.analyze_best_model(\n",
    "        #     all_results, y_train, original_train_for_analysis\n",
    "        # )\n",
    "# =========================================================================\n",
    "        # 修正開始\n",
    "        # =========================================================================\n",
    "        # 步驟 3: 確定最佳模型名稱並定義返回值 (取代 ErrorAnalyzer)\n",
    "        # 由於 run_experiment_tune 通常只訓練一個模型，我們直接取其名稱\n",
    "        best_model_name = list(all_results.keys())[0]\n",
    "        self.logger.info(f\"步驟 3: 最佳模型名稱確定為: {best_model_name}\")\n",
    "\n",
    "        # 錯誤分析已註解，必須定義返回變數作為預留位置\n",
    "        error_df = pd.DataFrame()\n",
    "        dashboard_figure = plt.figure()\n",
    "        # =========================================================================\n",
    "        # 修正結束\n",
    "        # =========================================================================\n",
    "        # 4. 生成提交文件\n",
    "        self.logger.info(\"步驟 4: 生成提交文件...\")\n",
    "        submission_df = self._generate_submission(\n",
    "            f\"submission_{best_model_name}_{feature_engineering_pipeline.__name__}.csv\",\n",
    "            test_ids,\n",
    "            all_results[best_model_name]['test_preds']\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"--- 調優成功完成 ---\")\n",
    "        return submission_df, all_results, error_df, dashboard_figure\n",
    "\n",
    "    def run_experiment(self,\n",
    "                       train_df: pd.DataFrame,\n",
    "                       test_df: pd.DataFrame,\n",
    "                       feature_engineering_pipeline: Callable,\n",
    "                       models: dict, # 使用 dict\n",
    "                       target_col: str = Config.TARGET_COL) -> tuple[pd.DataFrame, dict, pd.DataFrame, plt.Figure]: # 使用 tuple 和 dict\n",
    "        \"\"\"\n",
    "        啟動完整的實驗週期：特徵工程 (FE)、訓練、錯誤分析、生成提交文件。\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"--- 啟動新實驗 (特徵工程 FE: {feature_engineering_pipeline.__name__}) ---\")\n",
    "\n",
    "        test_ids = test_df['id'].copy()\n",
    "        original_train_for_analysis = train_df.copy()\n",
    "        y_train = train_df[target_col].astype(int)\n",
    "\n",
    "        # 1. 特徵工程\n",
    "        self.logger.info(\"步驟 1: 應用特徵工程...\")\n",
    "        X_train_processed = feature_engineering_pipeline(train_df, is_train=True)\n",
    "        X_test_processed = feature_engineering_pipeline(test_df, is_train=False)\n",
    "\n",
    "        train_cols = X_train_processed.columns\n",
    "        test_cols = X_test_processed.columns\n",
    "        if not train_cols.equals(test_cols):\n",
    "            self.logger.warning(\"訓練集和測試集的欄位不一致! 正在對齊...\")\n",
    "            shared_cols = list(train_cols.intersection(test_cols))\n",
    "            X_train_processed = X_train_processed[shared_cols]\n",
    "            X_test_processed = X_test_processed[shared_cols]\n",
    "\n",
    "        # 2. 訓練與評估模型\n",
    "        self.logger.info(\"步驟 2: 在交叉驗證上訓練模型...\")\n",
    "        all_results = self._evaluate_models(models, X_train_processed, y_train, X_test_processed)\n",
    "\n",
    "        # 3. 錯誤分析(暫時不用，因此要修正)\n",
    "        # self.logger.info(\"步驟 3: 分析最佳模型的錯誤...\")\n",
    "        # best_model_name, error_df, dashboard_figure = ErrorAnalyzer.analyze_best_model(\n",
    "        #     all_results, y_train, original_train_for_analysis\n",
    "        # )\n",
    "# =========================================================================\n",
    "        # 修正開始\n",
    "        # =========================================================================\n",
    "        # 步驟 3: 確定最佳模型名稱並定義返回值 (取代 ErrorAnalyzer)\n",
    "        self.logger.info(\"步驟 3: 確定性能最佳的模型名稱...\")\n",
    "        # 根據 CV ROC AUC 平均值選出最佳模型\n",
    "        best_roc_auc = -1.0\n",
    "        best_model_name = None\n",
    "        for name, result in all_results.items():\n",
    "            current_auc = result['metrics_df']['ROC AUC'].mean()\n",
    "            if current_auc > best_roc_auc:\n",
    "                best_roc_auc = current_auc\n",
    "                best_model_name = name\n",
    "\n",
    "        # 錯誤分析已註解，必須定義返回變數作為預留位置\n",
    "        error_df = pd.DataFrame()\n",
    "        dashboard_figure = plt.figure()\n",
    "        # =========================================================================\n",
    "        # 修正結束\n",
    "        # =========================================================================\n",
    "        # 4. 生成提交文件\n",
    "        self.logger.info(\"步驟 4: 生成提交文件...\")\n",
    "        submission_df = self._generate_submission(\n",
    "            f\"submission_{best_model_name}_{feature_engineering_pipeline.__name__}.csv\",\n",
    "            test_ids,\n",
    "            all_results[best_model_name]['test_preds']\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"--- 實驗成功完成 ---\")\n",
    "        return submission_df, all_results, error_df, dashboard_figure\n",
    "\n",
    "\n",
    "    def _evaluate_models(self, models: dict, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame) -> dict: # 使用 dict\n",
    "        \"\"\"\n",
    "        使用交叉驗證訓練和驗證模型 (僅保留 XGBoost 相關邏輯)。\n",
    "        \"\"\"\n",
    "        self.logger.info(\"啟動交叉驗證...\")\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        results = {}\n",
    "\n",
    "        # 即使只用 XGBoost，偵測類別特徵仍是重要的步驟\n",
    "        cat_feature_names = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "        if cat_feature_names:\n",
    "            self.logger.info(f\"偵測到類別特徵: {cat_feature_names}\")\n",
    "\n",
    "        for name, model in models.items():\n",
    "            self.logger.info(f\"正在訓練模型: {name}\")\n",
    "            oof_preds = np.zeros(len(X_train))\n",
    "            test_preds_folds, fold_metrics_list, importances_folds = [], [], []\n",
    "\n",
    "\n",
    "            # 進行 K 折交叉驗證\n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                current_model = clone(model)\n",
    "                fit_params = {}\n",
    "\n",
    "                X_tr_fit, X_val_fit = X_tr, X_val\n",
    "\n",
    "                # --- 簡化後的模型特定邏輯：僅保留 XGBClassifier ---\n",
    "                if isinstance(current_model, XGBClassifier):\n",
    "                    fit_params['eval_set'] = [(X_val_fit, y_val)]\n",
    "                    fit_params['verbose'] = False # 設置 XGBoost 靜默模式\n",
    "\n",
    "                # 提醒：若 models 字典中包含非 XGBClassifier 模型，它們將在這裡使用預設 fit 參數訓練\n",
    "                # 且若為非原生支持類別特徵的模型，且數據中包含類別特徵，將會訓練失敗。\n",
    "                # ----------------------------------------------------\n",
    "\n",
    "                # 訓練模型\n",
    "                current_model.fit(X_tr_fit, y_tr, **fit_params)\n",
    "\n",
    "                # 預測\n",
    "                X_test_predict = X_test.copy()\n",
    "                # 由於只保留 XGBoost，且假設 XGBoost 透過 enable_categorical=True 原生處理類別特徵，\n",
    "                # 我們移除手動編碼邏輯。\n",
    "\n",
    "                proba_val = current_model.predict_proba(X_val_fit)[:, 1] # 驗證集預測概率\n",
    "                proba_test = current_model.predict_proba(X_test_predict)[:, 1] # 測試集預測概率\n",
    "\n",
    "                oof_preds[val_idx] = proba_val\n",
    "                test_preds_folds.append(proba_test)\n",
    "\n",
    "                # 收集指標和特徵重要性\n",
    "                fold_metrics_list.append(\n",
    "                    {'ROC AUC': roc_auc_score(y_val, proba_val), 'PR AUC': average_precision_score(y_val, proba_val)})\n",
    "                if hasattr(current_model, 'feature_importances_'):\n",
    "                    importances_folds.append(current_model.feature_importances_) # 樹模型\n",
    "                elif hasattr(current_model, 'coef_'):\n",
    "                    importances_folds.append(np.abs(current_model.coef_[0])) # 線性模型\n",
    "\n",
    "            # 儲存結果\n",
    "            results[name] = {\n",
    "                'oof_preds': oof_preds,\n",
    "                'test_preds': np.mean(test_preds_folds, axis=0),\n",
    "                'metrics_df': pd.DataFrame(fold_metrics_list),\n",
    "                'feature_importances': np.mean(importances_folds, axis=0) if importances_folds else None,\n",
    "                'feature_names': X_train.columns\n",
    "            }\n",
    "            self.logger.info(\n",
    "                f\"  模型 {name} | CV ROC AUC: {results[name]['metrics_df']['ROC AUC'].mean():.4f} ± {results[name]['metrics_df']['ROC AUC'].std():.4f}\")\n",
    "        return results\n",
    "\n",
    "    def _generate_submission(self, filename: str, df_test_id: pd.Series, test_preds: np.ndarray) -> pd.DataFrame:\n",
    "        print(f'filename = {filename}')\n",
    "        # 保留這個特殊的文件名處理邏輯\n",
    "        if filename == 'submission_CatBoost_final_run_v3_preprocessing.csv':\n",
    "            filename = 'submission.csv'\n",
    "        print(f'filename1 = {filename}')\n",
    "        submission_df = pd.DataFrame({'id': df_test_id, 'Exited': test_preds})\n",
    "        submission_df.to_csv(filename, index=False)\n",
    "        self.logger.info(f\"提交文件成功保存: {filename}\")\n",
    "        return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('ModelTrainer')\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"超參數調優類別，使用 Optuna 進行優化。\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _objective(trial: optuna.Trial, X: pd.DataFrame, y: pd.Series, cat_feature_names: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Optuna 的目標函數：使用交叉驗證評估一組超參數。\n",
    "        此函數已修正為兼容 XGBoost 1.7.6 的 fit 語法。\n",
    "        \"\"\"\n",
    "        # 1. 定義要調優的 XGBoost 參數空間\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        }\n",
    "\n",
    "        # 2. 定義固定參數\n",
    "        fixed_params = {\n",
    "            'random_state': Config.RANDOM_STATE,\n",
    "            'verbose': 0,\n",
    "            'eval_metric': 'logloss',\n",
    "            'n_jobs': -1,\n",
    "            'early_stopping_rounds': 50, # 舊版早停參數，必須在模型實例化時傳入\n",
    "            'enable_categorical': True, # 啟用原生類別特徵支持 (如果 XGBoost 版本夠新)\n",
    "        }\n",
    "\n",
    "        full_params = {**params, **fixed_params}\n",
    "\n",
    "        # 3. 創建模型\n",
    "        model = XGBClassifier(**full_params)\n",
    "\n",
    "        # 4. 交叉驗證與擬合\n",
    "        skf = StratifiedKFold(n_splits=Config.N_SPLITS, shuffle=True, random_state=fixed_params['random_state'])\n",
    "        roc_auc_scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            # --- 兼容 XGBoost 1.7.6 的 fit 參數 ---\n",
    "            fit_params = {\n",
    "                'eval_set': [(X_val, y_val)], # 必須在 fit 中傳遞驗證集\n",
    "                'verbose': False\n",
    "            }\n",
    "            # 注意：這裡不能傳遞 'callbacks' 參數\n",
    "\n",
    "            try:\n",
    "                model.fit(X_tr, y_tr, **fit_params)\n",
    "\n",
    "                # 確保我們使用訓練完成的模型進行預測\n",
    "                best_iteration = model.get_booster().best_iteration\n",
    "\n",
    "                # 使用最佳迭代次數預測\n",
    "                proba_val = model.predict_proba(X_val, iteration_range=(0, best_iteration))[:, 1]\n",
    "                roc_auc_scores.append(roc_auc_score(y_val, proba_val))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Optuna Fold {fold} 訓練失敗: {e}\")\n",
    "                # 遇到錯誤時，返回 0.0 或拋出 PruningError 讓 Optuna 處理\n",
    "\n",
    "        # 5. 返回平均 ROC AUC 分數\n",
    "        return np.mean(roc_auc_scores)\n",
    "\n",
    "    @staticmethod\n",
    "    def tune(X: pd.DataFrame, y: pd.Series, cat_feature_names: List[str], n_trials: int) -> dict:\n",
    "        \"\"\"執行 Optuna 調優並返回最佳參數。\"\"\"\n",
    "        # 創建 Optuna 研究 (Study)\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "\n",
    "        # 包裝目標函數，傳遞數據\n",
    "        objective_with_args = lambda trial: HyperparameterTuner._objective(trial, X, y, cat_feature_names)\n",
    "\n",
    "        # 開始優化\n",
    "        study.optimize(objective_with_args, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "        print(f\"調優完成。最佳 ROC AUC: {study.best_value:.5f}\")\n",
    "        print(\"最佳參數:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"協調器類別，用於統一模型訓練、評估和預測的流程。\"\"\"\n",
    "\n",
    "    def __init__(self, n_splits: int = Config.N_SPLITS, random_state: int = Config.RANDOM_STATE):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "        if not self.logger.handlers:\n",
    "            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    def run_experiment_tune(self,\n",
    "                           train_df: pd.DataFrame,\n",
    "                           test_df: pd.DataFrame,\n",
    "                           feature_engineering_pipeline: Callable,\n",
    "                           models: Dict[str, Any],\n",
    "                           target_col: str = Config.TARGET_COL,\n",
    "                           tune_hyperparams: bool = False,\n",
    "                           tune_model_name: str = 'XGBoost',\n",
    "                           n_trials: int = 50) -> Tuple[pd.DataFrame, Dict, pd.DataFrame, plt.Figure]:\n",
    "        \"\"\"啟動完整的實驗週期，可選配超參數調優。\"\"\"\n",
    "        self.logger.info(f\"--- 啟動新實驗 (特徵工程 FE: {feature_engineering_pipeline.__name__}) ---\")\n",
    "\n",
    "        if train_df.empty:\n",
    "            self.logger.error(\"訓練數據為空，無法運行實驗。\")\n",
    "            return pd.DataFrame(), {}, pd.DataFrame(), plt.figure()\n",
    "\n",
    "\n",
    "        test_ids = test_df['id'].copy()\n",
    "        original_train_for_analysis = train_df.copy()\n",
    "        y_train = train_df[target_col].astype(int)\n",
    "\n",
    "        self.logger.info(\"步驟 1: 應用特徵工程...\")\n",
    "        X_train_processed = feature_engineering_pipeline(train_df, is_train=True)\n",
    "        X_test_processed = feature_engineering_pipeline(test_df, is_train=False)\n",
    "\n",
    "        # 對齊欄位\n",
    "        train_cols = X_train_processed.columns\n",
    "        test_cols = X_test_processed.columns\n",
    "        if not train_cols.equals(test_cols):\n",
    "            self.logger.warning(\"訓練集和測試集的欄位不一致! 正在對齊...\")\n",
    "            shared_cols = list(train_cols.intersection(test_cols))\n",
    "            X_train_processed = X_train_processed[shared_cols]\n",
    "            X_test_processed = X_test_processed[shared_cols]\n",
    "\n",
    "        models_to_train = models.copy()\n",
    "        \n",
    "        # 步驟 1.5: 超參數調優\n",
    "        if tune_hyperparams:\n",
    "            self.logger.info(f\"!!! 已為模型 '{tune_model_name}' 啟用超參數調優模式 !!!\")\n",
    "            if tune_model_name not in models:\n",
    "                self.logger.error(f\"用於調優的模型 '{tune_model_name}' 未在 models 字典中找到。調優已取消。\")\n",
    "            else:\n",
    "                self.logger.info(f\"步驟 1.5: 為 '{tune_model_name}' 進行超參數調優...\")\n",
    "\n",
    "                cat_features = X_train_processed.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "                best_params = HyperparameterTuner.tune(\n",
    "                    X=X_train_processed,\n",
    "                    y=y_train,\n",
    "                    cat_feature_names=cat_features,\n",
    "                    n_trials=n_trials\n",
    "                )\n",
    "\n",
    "                # 重新整合參數\n",
    "                best_params.update({\n",
    "                    'random_state': self.random_state,\n",
    "                    'eval_metric': 'logloss',\n",
    "                    'n_jobs': -1,\n",
    "                    # early_stopping_rounds 必須在模型實例化時傳入\n",
    "                    'early_stopping_rounds': best_params.get('early_stopping_rounds', 50), \n",
    "                    'enable_categorical': True,\n",
    "                    'verbose': 0\n",
    "                })\n",
    "                \n",
    "                # 實例化調整後的模型\n",
    "                tuned_model = XGBClassifier(**best_params)\n",
    "                tuned_model_name = f\"{tune_model_name}_Tuned\"\n",
    "                models_to_train = {tuned_model_name: tuned_model}\n",
    "                self.logger.info(f\"調優完成。模型 '{tuned_model_name}' 將用於訓練。\")\n",
    "\n",
    "        # 2. 訓練與評估模型\n",
    "        self.logger.info(\"步驟 2: 在交叉驗證上訓練模型...\")\n",
    "        all_results = self._evaluate_models(models_to_train, X_train_processed, y_train, X_test_processed)\n",
    "\n",
    "        # 3. 確定最佳模型名稱並定義返回值 (取代 ErrorAnalyzer)\n",
    "        best_model_name = list(all_results.keys())[0] # 如果只訓練一個模型，就是它\n",
    "        self.logger.info(f\"步驟 3: 最佳模型名稱確定為: {best_model_name}\")\n",
    "        error_df = pd.DataFrame()\n",
    "        dashboard_figure = plt.figure()\n",
    "\n",
    "        # 4. 生成提交文件\n",
    "        self.logger.info(\"步驟 4: 生成提交文件...\")\n",
    "        submission_df = self._generate_submission(\n",
    "            f\"submission_{best_model_name}_{feature_engineering_pipeline.__name__}.csv\",\n",
    "            test_ids,\n",
    "            all_results[best_model_name]['test_preds']\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"--- 調優成功完成 ---\")\n",
    "        return submission_df, all_results, error_df, dashboard_figure\n",
    "\n",
    "    def run_experiment(self,\n",
    "                       train_df: pd.DataFrame,\n",
    "                       test_df: pd.DataFrame,\n",
    "                       feature_engineering_pipeline: Callable,\n",
    "                       models: Dict[str, Any],\n",
    "                       target_col: str = Config.TARGET_COL) -> Tuple[pd.DataFrame, Dict, pd.DataFrame, plt.Figure]:\n",
    "        \"\"\"啟動完整的實驗週期：特徵工程 (FE)、訓練、生成提交文件。\"\"\"\n",
    "        self.logger.info(f\"--- 啟動新實驗 (特徵工程 FE: {feature_engineering_pipeline.__name__}) ---\")\n",
    "\n",
    "        if train_df.empty:\n",
    "            self.logger.error(\"訓練數據為空，無法運行實驗。\")\n",
    "            return pd.DataFrame(), {}, pd.DataFrame(), plt.figure()\n",
    "\n",
    "        test_ids = test_df['id'].copy()\n",
    "        original_train_for_analysis = train_df.copy()\n",
    "        y_train = train_df[target_col].astype(int)\n",
    "\n",
    "        # 1. 特徵工程\n",
    "        self.logger.info(\"步驟 1: 應用特徵工程...\")\n",
    "        X_train_processed = feature_engineering_pipeline(train_df, is_train=True)\n",
    "        X_test_processed = feature_engineering_pipeline(test_df, is_train=False)\n",
    "\n",
    "        # 對齊欄位\n",
    "        train_cols = X_train_processed.columns\n",
    "        test_cols = X_test_processed.columns\n",
    "        if not train_cols.equals(test_cols):\n",
    "            self.logger.warning(\"訓練集和測試集的欄位不一致! 正在對齊...\")\n",
    "            shared_cols = list(train_cols.intersection(test_cols))\n",
    "            X_train_processed = X_train_processed[shared_cols]\n",
    "            X_test_processed = X_test_processed[shared_cols]\n",
    "\n",
    "        # 2. 訓練與評估模型\n",
    "        self.logger.info(\"步驟 2: 在交叉驗證上訓練模型...\")\n",
    "        all_results = self._evaluate_models(models, X_train_processed, y_train, X_test_processed)\n",
    "\n",
    "        # 3. 確定最佳模型名稱並定義返回值 (取代 ErrorAnalyzer)\n",
    "        self.logger.info(\"步驟 3: 確定性能最佳的模型名稱...\")\n",
    "        best_roc_auc = -1.0\n",
    "        best_model_name = None\n",
    "        for name, result in all_results.items():\n",
    "            current_auc = result['metrics_df']['ROC AUC'].mean()\n",
    "            if current_auc > best_roc_auc:\n",
    "                best_roc_auc = current_auc\n",
    "                best_model_name = name\n",
    "\n",
    "        if best_model_name is None:\n",
    "            self.logger.error(\"沒有模型成功訓練或評估。\")\n",
    "            return pd.DataFrame(), all_results, pd.DataFrame(), plt.figure()\n",
    "\n",
    "\n",
    "        error_df = pd.DataFrame()\n",
    "        dashboard_figure = plt.figure()\n",
    "\n",
    "        # 4. 生成提交文件\n",
    "        self.logger.info(\"步驟 4: 生成提交文件...\")\n",
    "        submission_df = self._generate_submission(\n",
    "            f\"submission_{best_model_name}_{feature_engineering_pipeline.__name__}.csv\",\n",
    "            test_ids,\n",
    "            all_results[best_model_name]['test_preds']\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"--- 實驗成功完成 ---\")\n",
    "        return submission_df, all_results, error_df, dashboard_figure\n",
    "\n",
    "    def _evaluate_models(self, models: Dict[str, Any], X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        使用交叉驗證訓練和驗證模型。\n",
    "        此函數已修正為兼容 XGBoost 1.7.6 的 fit 語法。\n",
    "        \"\"\"\n",
    "        self.logger.info(\"啟動交叉驗證...\")\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        results = {}\n",
    "\n",
    "        cat_feature_names = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "        if cat_feature_names:\n",
    "            self.logger.info(f\"偵測到類別特徵: {cat_feature_names}\")\n",
    "\n",
    "        for name, model in models.items():\n",
    "            self.logger.info(f\"正在訓練模型: {name}\")\n",
    "            oof_preds = np.zeros(len(X_train))\n",
    "            test_preds_folds, fold_metrics_list, importances_folds = [], [], []\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                current_model = clone(model)\n",
    "                fit_params = {}\n",
    "\n",
    "                # --- 關鍵修正區塊：兼容 XGBoost 1.7.6 ---\n",
    "                if isinstance(current_model, XGBClassifier):\n",
    "                    # 舊版 XGBoost 需要 eval_set 作為 fit() 參數\n",
    "                    fit_params['eval_set'] = [(X_val, y_val)]\n",
    "                    fit_params['verbose'] = False # 設置靜默模式\n",
    "                    # early_stopping_rounds 必須在模型實例化時傳入 \n",
    "                # ----------------------------------------\n",
    "\n",
    "                try:\n",
    "                    # 訓練模型\n",
    "                    current_model.fit(X_tr, y_tr, **fit_params)\n",
    "\n",
    "                    # 確保我們使用最佳迭代次數預測\n",
    "                    best_iteration = current_model.get_booster().best_iteration\n",
    "\n",
    "                    # 預測驗證集\n",
    "                    proba_val = current_model.predict_proba(X_val, iteration_range=(0, best_iteration))[:, 1]\n",
    "                    # 預測測試集\n",
    "                    proba_test = current_model.predict_proba(X_test, iteration_range=(0, best_iteration))[:, 1]\n",
    "\n",
    "                    oof_preds[val_idx] = proba_val\n",
    "                    test_preds_folds.append(proba_test)\n",
    "\n",
    "                    # 收集指標\n",
    "                    fold_metrics_list.append(\n",
    "                        {'ROC AUC': roc_auc_score(y_val, proba_val), 'PR AUC': average_precision_score(y_val, proba_val)})\n",
    "                    \n",
    "                    # 收集特徵重要性 (假設是樹模型)\n",
    "                    if hasattr(current_model, 'feature_importances_'):\n",
    "                        importances_folds.append(current_model.feature_importances_)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"模型 {name} 在折疊 {fold} 訓練時發生錯誤: {e}\")\n",
    "                    # 如果訓練失敗，則跳過此折疊的預測和指標收集\n",
    "                    continue\n",
    "\n",
    "            # 儲存結果\n",
    "            results[name] = {\n",
    "                'oof_preds': oof_preds,\n",
    "                'test_preds': np.mean(test_preds_folds, axis=0) if test_preds_folds else np.zeros(len(X_test)),\n",
    "                'metrics_df': pd.DataFrame(fold_metrics_list),\n",
    "                'feature_importances': np.mean(importances_folds, axis=0) if importances_folds else None,\n",
    "                'feature_names': X_train.columns\n",
    "            }\n",
    "            if not results[name]['metrics_df'].empty:\n",
    "                self.logger.info(\n",
    "                    f\" 模型 {name} | CV ROC AUC: {results[name]['metrics_df']['ROC AUC'].mean():.4f} ± {results[name]['metrics_df']['ROC AUC'].std():.4f}\")\n",
    "            else:\n",
    "                 self.logger.warning(f\"模型 {name} 訓練失敗，無法計算 CV ROC AUC。\")\n",
    "        return results\n",
    "\n",
    "    def _generate_submission(self, filename: str, df_test_id: pd.Series, test_preds: np.ndarray) -> pd.DataFrame:\n",
    "        # 由於您原代碼中有一個特定的替換邏輯，為防止意外，我將其保留。\n",
    "        # 除非有必要，否則應避免這種硬編碼的檔名替換。\n",
    "        # print(f'filename = {filename}')\n",
    "        # if filename == 'submission_CatBoost_final_run_v3_preprocessing.csv':\n",
    "        #     filename = 'submission.csv'\n",
    "        # print(f'filename1 = {filename}')\n",
    "        \n",
    "        submission_df = pd.DataFrame({'id': df_test_id, 'Exited': test_preds})\n",
    "        # 由於 notebook 環境中，我們將檔案儲存在本地\n",
    "        submission_df.to_csv(filename, index=False)\n",
    "        self.logger.info(f\"提交文件成功保存: {filename}\")\n",
    "        return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_train.empty:\n",
    "    trainer = ModelTrainer()\n",
    "\n",
    "    # 整合 Optuna 找到的最佳參數和固定的 XGBoost 參數\n",
    "    # 這些參數是從假設的 Optuna 運行中獲得的優化結果\n",
    "    final_best_params = {\n",
    "        # Optuna 最佳參數 (範例)\n",
    "        'n_estimators': 2692,\n",
    "        'learning_rate': 0.05786197845936901,\n",
    "        'max_depth': 3,\n",
    "        'reg_lambda': 1.0628185137032307e-08,\n",
    "        'reg_alpha': 3.255737505871401,\n",
    "        'subsample': 0.8409191153520594,\n",
    "        'colsample_bytree': 0.7834673458794292,\n",
    "\n",
    "        # 固定的參數 (確保兼容性)\n",
    "        'random_state': Config.RANDOM_STATE,\n",
    "        'eval_metric': 'logloss',\n",
    "        'n_jobs': -1,\n",
    "        'early_stopping_rounds': 50,  # 舊版早停參數\n",
    "        'enable_categorical': True, \n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # 實例化最終模型\n",
    "    final_tuned_model = XGBClassifier(**final_best_params)\n",
    "\n",
    "    # 創建包含最終模型的字典\n",
    "    models_final = {\n",
    "        'XGBoost_Final_Tuned': final_tuned_model\n",
    "    }\n",
    "\n",
    "    # 選擇最佳特徵工程版本\n",
    "    best_fe_pipeline = FeatureEngineer.run_v2_preprocessing\n",
    "\n",
    "    # 運行最終實驗\n",
    "    submission_final, results_final, errors_final, dashboard_final = trainer.run_experiment(\n",
    "        train_df=df_train,\n",
    "        test_df=df_test,\n",
    "        feature_engineering_pipeline=best_fe_pipeline,\n",
    "        models=models_final\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- 最終結果摘要 ---\")\n",
    "    if 'XGBoost_Final_Tuned' in results_final:\n",
    "        metrics_df = results_final['XGBoost_Final_Tuned']['metrics_df']\n",
    "        print(f\"模型: XGBoost_Final_Tuned (FE: {best_fe_pipeline.__name__})\")\n",
    "        print(f\"交叉驗證 ROC AUC: {metrics_df['ROC AUC'].mean():.5f} ± {metrics_df['ROC AUC'].std():.5f}\")\n",
    "    \n",
    "    print(f\"提交文件已保存為: submission_XGBoost_Final_Tuned_{best_fe_pipeline.__name__}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示交叉驗證結果\n",
    "if 'XGBoost_Final_Tuned' in results_final:\n",
    "    metrics_df = results_final['XGBoost_Final_Tuned']['metrics_df']\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=metrics_df.index, y='ROC AUC', data=metrics_df, palette='viridis')\n",
    "    plt.title('Fold ROC AUC Scores')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('ROC AUC Score')\n",
    "    plt.show()\n",
    "\n",
    "# 展示特徵重要性\n",
    "if 'XGBoost_Final_Tuned' in results_final and results_final['XGBoost_Final_Tuned']['feature_importances'] is not None:\n",
    "    feature_importances = results_final['XGBoost_Final_Tuned']['feature_importances']\n",
    "    feature_names = results_final['XGBoost_Final_Tuned']['feature_names']\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20), palette='magma')\n",
    "    plt.title('Top 20 Feature Importances (Mean across Folds)')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOBNKkESGGETYGJP2B0Wnf9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AI_WEB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
