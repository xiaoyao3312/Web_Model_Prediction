# services\customer_churn_bank_service.py
import pandas as pd
import numpy as np
import logging
import joblib
import shap
import os
import sys  # ğŸš¨ å°å…¥ sys ç”¨æ–¼å¼·åˆ¶æ‰“å°åˆ° stderr

from typing import Dict, Any, List, Callable

# ğŸš¨ ç‚ºäº†è®“æœå‹™èƒ½ç¨ç«‹é‹è¡Œï¼Œæˆ‘å€‘ä¸ç›´æ¥å¾ train.py å°å…¥ FeatureEngineerï¼Œè€Œæ˜¯å‡è¨­
# å¤–éƒ¨æœƒæä¾› FE å‡½æ•¸ï¼ˆä¾‹å¦‚ routes.py ä¸­çš„ FeatureEngineerForAPIï¼‰

logger = logging.getLogger('CustomerChurnBankService')
logger.setLevel(logging.INFO)

class CustomerChurnBankService:
    def __init__(self, model_path: str, model_dir: str):
        # ğŸš¨ _load_model è£¡é¢ç¾åœ¨æœ‰å¼·åˆ¶éŒ¯èª¤è™•ç†
        self.model = self._load_model(model_path)
        # ğŸš¨ [æ–°å¢] å¦‚æœæ¨¡å‹æˆåŠŸè¼‰å…¥ï¼Œæ‰“å°æˆåŠŸè¨Šæ¯
        if self.model is not None:
            logger.info("æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œæº–å‚™åˆå§‹åŒ– SHAP Explainerã€‚") # ğŸš¨ æ–°å¢
        self.model_dir = model_dir
        
        # è¼‰å…¥è¨“ç·´æ™‚ä¿å­˜çš„ç‰¹å¾µåˆ—è¡¨å’Œ FE ç®¡é“åç¨±
        self.feature_cols, self.fe_pipeline_name = self._load_model_artifacts(model_dir)
        
        # å»ºç«‹ SHAP Explainer (åœ¨æœå‹™å•Ÿå‹•æ™‚ä¸€æ¬¡æ€§å®Œæˆ)
        if self.model:
            try:
                # ğŸš¨ [ä¿®æ”¹] æš«æ™‚è¨»é‡‹æ‰ SHAP åˆå§‹åŒ–ï¼Œä»¥ç¢ºèªè¼‰å…¥æ˜¯å¦æˆåŠŸ
                self.explainer = shap.TreeExplainer(self.model) 
                logger.info("SHAP TreeExplainer æˆåŠŸåˆå§‹åŒ–ã€‚")
                
                # # ğŸš¨ [æ–°å¢] è‡¨æ™‚è¨­å®š Explainer ç‚º Noneï¼Œä¸¦æ‰“å°è·³éè¨Šæ¯
                # self.explainer = None
                # logger.warning("!!! SHAP åˆå§‹åŒ–æš«æ™‚è·³éï¼Œç”¨æ–¼æ¨¡å‹è¼‰å…¥æ¸¬è©¦ !!!") 
            
            except Exception as e:
                # ğŸš¨ ã€é‡è¦ã€‘å¦‚æœ SHAP å¤±æ•—ï¼Œæ‰“å°åš´é‡éŒ¯èª¤
                print(f"!!! åš´é‡éŒ¯èª¤ !!! SHAP åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr) 
                raise RuntimeError(f"SHAP åˆå§‹åŒ–å¤±æ•—ï¼Œæœå‹™ç„¡æ³•å•Ÿå‹•: {e}") 
        else:
            # é€™æ‡‰è©²åœ¨ _load_model è£¡é¢å·²ç¶“è™•ç†ï¼Œä½†ä½œç‚ºæœ€çµ‚ä¿éšœ
            raise RuntimeError("æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•åˆå§‹åŒ–æœå‹™ã€‚")

    def _load_model_artifacts(self, model_dir: str) -> tuple[List[str], str]:
        """è¼‰å…¥è¨“ç·´è…³æœ¬ç”¢ç”Ÿçš„ç‰¹å¾µåˆ—è¡¨å’Œ FE ç®¡é“åç¨±ã€‚"""
        feature_cols_path = os.path.join(model_dir, 'feature_columns.joblib')
        fe_name_path = os.path.join(model_dir, 'fe_pipeline_name.txt')
        
        # ğŸš¨ å¼·åˆ¶æ‰“å°è·¯å¾‘ï¼Œç¢ºä¿é€™äº›æª”æ¡ˆå·¥ä»¶è·¯å¾‘ä¹Ÿæ²’å•é¡Œ
        print(f"DEBUG: å˜—è©¦è¼‰å…¥ç‰¹å¾µæ¬„ä½è·¯å¾‘: {feature_cols_path}", file=sys.stderr)
        print(f"DEBUG: å˜—è©¦è¼‰å…¥ FE åç¨±è·¯å¾‘: {fe_name_path}", file=sys.stderr)
        
        if not os.path.exists(feature_cols_path) or not os.path.exists(fe_name_path):
            logger.warning(f"æ¨¡å‹å·¥ä»¶ (feature_columns/fe_pipeline_name) æœªæ‰¾åˆ°æ–¼ {model_dir}")
            return [], "" # è¿”å›ç©ºåˆ—è¡¨å’Œç©ºå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿å¾ŒçºŒä½¿ç”¨æ¨¡æ“¬é æ¸¬

        try:
            feature_cols = joblib.load(feature_cols_path)
            with open(fe_name_path, 'r') as f:
                fe_pipeline_name = f.read().strip()
            logger.info(f"ç‰¹å¾µåˆ— ({len(feature_cols)}) å’Œ FE ç®¡é“åç¨± ({fe_pipeline_name}) è¼‰å…¥æˆåŠŸã€‚")
            return feature_cols, fe_pipeline_name
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¨¡å‹å·¥ä»¶å¤±æ•—: {e}", exc_info=True)
            # ğŸš¨ é‡åˆ°éŒ¯èª¤ï¼Œå¼·åˆ¶æ‹‹å‡º
            raise RuntimeError(f"æ¨¡å‹å·¥ä»¶è¼‰å…¥è‡´å‘½éŒ¯èª¤. åŸå› : {e}") from e

    def _load_model(self, model_path: str) -> Any:
        """è¼‰å…¥é è¨“ç·´çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ã€‚"""
        # ğŸš¨ å¼·åˆ¶æ‰“å°è·¯å¾‘ï¼Œç¢ºä¿å³ä½¿æ˜¯ Worker Process ä¹Ÿèƒ½å°‡æ­¤ä¿¡æ¯è¼¸å‡ºåˆ° Render æ—¥èªŒ
        print(f"DEBUG: å˜—è©¦è¼‰å…¥æ¨¡å‹è·¯å¾‘: {model_path}", file=sys.stderr) 

        if not os.path.exists(model_path):
            logger.error(f"!!! åš´é‡éŒ¯èª¤ !!! æ¨¡å‹æª”æ¡ˆæœªæ‰¾åˆ°: {model_path}")
            # ğŸš¨ é‡åˆ°éŒ¯èª¤ï¼Œå¼·åˆ¶æ‹‹å‡º FileNotFoundError
            raise FileNotFoundError(f"æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨æ–¼æŒ‡å®šè·¯å¾‘: {model_path}")
        
        try:
            model = joblib.load(model_path)
            logger.info(f"æ¨¡å‹ {model_path} è¼‰å…¥æˆåŠŸã€‚")
            return model
        except Exception as e:
            logger.error(f"!!! åš´é‡éŒ¯èª¤ !!! è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}", exc_info=True)
            # ğŸš¨ é‡åˆ°éŒ¯èª¤ï¼Œå¼·åˆ¶æ‹‹å‡º RuntimeError
            raise RuntimeError(f"æ¨¡å‹è¼‰å…¥è‡´å‘½éŒ¯èª¤: {model_path} è¼‰å…¥å¤±æ•—. åŸå› : {e}") from e


    def _align_features(self, df_processed: pd.DataFrame) -> pd.DataFrame:
        """æ ¹æ“šè¨“ç·´æ™‚çš„ç‰¹å¾µåˆ—è¡¨é€²è¡Œ OHE å’Œæ¬„ä½å°é½Šã€‚"""
        if not self.feature_cols:
            raise RuntimeError("ç‰¹å¾µæ¬„ä½åˆ—è¡¨æœªè¼‰å…¥ã€‚")

        # æ‰¾å‡ºé¡åˆ¥æ¬„ä½ (Geography, Age_bin ç­‰)
        # 'category' é¡å‹æ˜¯ pandas æ¨è–¦çš„ FE è¼¸å‡ºé¡å‹
        cat_cols = [col for col in df_processed.columns if df_processed[col].dtype.name in ['object', 'str', 'category']]
        
        # å°æ•¸æ“šé€²è¡Œ One-Hot Encoding
        X_oh = pd.get_dummies(df_processed, columns=cat_cols, dummy_na=False)
        
        # è£œé½Šè¨“ç·´é›†ç¼ºå°‘çš„æ¬„ä½ (ç•¶å‰å–®ä¸€è«‹æ±‚å¯èƒ½ç¼ºå°‘æŸå€‹ OHE æ¬„ä½)
        missing_cols = set(self.feature_cols) - set(X_oh.columns)
        for c in missing_cols:
            X_oh[c] = 0.0
        
        # ç§»é™¤å¤šé¤˜çš„æ¬„ä½ï¼Œä¸¦ç¢ºä¿é †åºä¸€è‡´
        # é€™ä¸€æ­¥æ˜¯é—œéµï¼šç¢ºä¿é æ¸¬æ•¸æ“šçš„æ¬„ä½åç¨±å’Œé †åºèˆ‡è¨“ç·´æ¨¡å‹æ™‚å®Œå…¨ç›¸åŒ
        X_predict = X_oh[[col for col in self.feature_cols if col in X_oh.columns]]
        X_predict = X_predict.astype(float)

        if X_predict.shape[1] != len(self.feature_cols):
            # åªæœ‰ç•¶ç¼ºå¤±çš„æ¬„ä½ä¸åœ¨ X_oh.columns ä¹Ÿä¸åœ¨ feature_cols ä¸­æ™‚æ‰æœƒå‡ºéŒ¯ï¼Œä½†ç‚ºäº†å¥å£¯æ€§ä¿ç•™
            raise ValueError(f"ç‰¹å¾µæ•¸é‡ä¸åŒ¹é…ã€‚é æœŸ {len(self.feature_cols)}ï¼Œå¯¦éš› {X_predict.shape[1]}")
        
        return X_predict

    def get_local_shap(self, X_predict: pd.DataFrame) -> Dict[str, float]:
        """è¨ˆç®—å–®ä¸€æ¨£æœ¬çš„å±€éƒ¨ SHAP å€¼ï¼Œä¸¦è½‰æ›ç‚ºå¯è®€çš„å­—å…¸ã€‚"""
        if not self.explainer:
            return {} # Explainer æœªåˆå§‹åŒ–å‰‡è¿”å›ç©º

        try:
            # è¨ˆç®— SHAP å€¼
            # shap_values å¯èƒ½æ˜¯ (1, num_features) çš„ numpy array
            # ç”±æ–¼ X_predict æ˜¯ä¸€å€‹å–®è¡Œ DataFrameï¼Œé€™è£¡çš„è¨ˆç®—çµæœæ‡‰è©²æ˜¯å–®ä¸€æ¨£æœ¬çš„
            shap_values = self.explainer.shap_values(X_predict, check_additivity=False)
            
            # ç”±æ–¼ XGBoost æ˜¯äºŒåˆ†é¡ï¼Œshap_values æ˜¯å…©å€‹é™£åˆ—çš„åˆ—è¡¨ (list of arrays)ï¼Œå–é¡åˆ¥ 1 çš„å€¼
            # ç¢ºä¿ shap_values_row æ˜¯ä¸€å€‹ä¸€ç¶­é™£åˆ— (å°æ–¼å–®è¡Œè¼¸å…¥)
            shap_values_row = shap_values[1][0] if isinstance(shap_values, list) and len(shap_values) == 2 else shap_values[0]
            
            # å¦‚æœæ˜¯å–®ä¸€æ¨£æœ¬ï¼Œç¢ºä¿æ˜¯å¾äºŒç¶­é™£åˆ—ä¸­å–å‡ºä¸€ç¶­æ•¸çµ„
            if len(shap_values_row.shape) > 1 and shap_values_row.shape[0] == 1:
                shap_values_row = shap_values_row[0]


            feature_names = X_predict.columns
            # ç¢ºä¿é•·åº¦åŒ¹é…
            if len(feature_names) != len(shap_values_row):
                 logger.error(f"SHAP å€¼æ•¸é‡ ({len(shap_values_row)}) èˆ‡ç‰¹å¾µæ•¸é‡ ({len(feature_names)}) ä¸åŒ¹é…ã€‚")
                 return {}


            shap_dict = dict(zip(feature_names, shap_values_row))
            
            # æ’åº (ä»¥ SHAP å€¼çš„çµ•å°å€¼é™åºæ’åˆ—)
            sorted_shap = dict(sorted(shap_dict.items(), key=lambda item: abs(item[1]), reverse=True))
            
            # ç‚ºäº†ç°¡åŒ– API è¼¸å‡ºï¼Œæˆ‘å€‘åªè¿”å›å‰ 7 å€‹æœ€æœ‰å½±éŸ¿åŠ›çš„ç‰¹å¾µ
            top_n_shap = {k: float(v) for k, v in list(sorted_shap.items())[:7]}
            
            return top_n_shap
        except Exception as e:
            logger.error(f"è¨ˆç®—å±€éƒ¨ SHAP å€¼å¤±æ•—: {e}")
            return {}


    def preprocess_and_predict(self, input_df: pd.DataFrame, fe_pipeline_func: Callable) -> Dict[str, Any]:
        """
        è™•ç†è¼¸å…¥æ•¸æ“šï¼Œé€²è¡Œç‰¹å¾µå·¥ç¨‹ï¼Œç„¶å¾Œé€²è¡Œå–®ä¸€é æ¸¬ã€‚
        
        Args:
            input_df: å·²ç¶“åŒ…å«åŸå§‹ç‰¹å¾µçš„ DataFrame (å–®è¡Œ)ã€‚
            fe_pipeline_func: å¾ routes å±¤å‚³éä¸‹ä¾†çš„ FE å‡½æ•¸ (e.g., run_v2_preprocessing)ã€‚
            
        Returns:
            åŒ…å«é æ¸¬çµæœã€é¢¨éšªå’Œå±€éƒ¨ SHAP æ•¸æ“šçš„å­—å…¸ã€‚
        """
        
        if self.model is None:
            # è¿”å›æ¨¡æ“¬çµæœ
            return {
                "prediction": 0,
                "probability": 0.5,
                "feature_importance": "æ¨¡å‹æœå‹™æœªå•Ÿå‹•ï¼Œä½¿ç”¨æ¨¡æ“¬é æ¸¬ã€‚",
                "local_shap_values": {}
            }
        
        # 1. ç‰¹å¾µå·¥ç¨‹ (ä½¿ç”¨ routes å±¤æä¾›çš„ FE å‡½æ•¸)
        processed_df = fe_pipeline_func(input_df.copy())
        
        # 2. OHE å’Œç‰¹å¾µå°é½Š
        X_predict = self._align_features(processed_df)

        # 3. é€²è¡Œé æ¸¬
        # predict_proba è¿”å›çš„æ˜¯ (n_samples, n_classes)ï¼Œå–ç¬¬äºŒå€‹é¡åˆ¥ (æµå¤±) çš„é¢¨éšª
        probability_class_1 = self.model.predict_proba(X_predict)[:, 1][0]
        prediction = int(probability_class_1 >= 0.5)

        # 4. é€²è¡Œå±€éƒ¨ SHAP åˆ†æ
        local_shap_values = self.get_local_shap(X_predict)
        
        # 5. è½‰æ›ç‚ºå¯è®€çš„ç‰¹å¾µé‡è¦æ€§æ–‡æœ¬ (ç”¨æ–¼ AI è§£é‡‹)
        feature_importance_text = "ä¸»è¦å½±éŸ¿å› ç´  (å±€éƒ¨ SHAP å€¼):\n"
        if local_shap_values:
            for feature, shap_value in local_shap_values.items():
                # SHAP å€¼ > 0 è¡¨ç¤ºæ¨é«˜æµå¤±é¢¨éšª
                sign = "æ¨é«˜æµå¤±é¢¨éšª (+)" if shap_value > 0 else "æ¨ä½æµå¤±é¢¨éšª (-)"
                feature_importance_text += f"- {feature}: {sign} (å½±éŸ¿å€¼: {abs(shap_value):.4f})\n"
        else:
            feature_importance_text = "SHAP åˆ†æå·¥å…·æœªæˆåŠŸåˆå§‹åŒ–æˆ–è¨ˆç®—å¤±æ•—ã€‚"

        return {
            "prediction": prediction,
            "probability": float(probability_class_1),
            "feature_importance": feature_importance_text,
            "local_shap_values": local_shap_values
        }
    
    def predict_batch_csv(self, input_df: pd.DataFrame, fe_pipeline_func: Callable) -> pd.DataFrame:
        """
        å°æ‰¹æ¬¡ CSV æ•¸æ“šé€²è¡Œé æ¸¬ï¼Œä¸¦è¿”å›å¸¶æœ‰é æ¸¬çµæœçš„ DataFrameã€‚
        
        Args:
            input_df: åŸå§‹å®¢æˆ¶æ•¸æ“šçš„ DataFrameã€‚
            fe_pipeline_func: ä¾†è‡ª routes å±¤çš„ç‰¹å¾µå·¥ç¨‹å‡½æ•¸ã€‚
            
        Returns:
            DataFrame: åŒ…å«åŸå§‹æ•¸æ“šå’Œ 'Exited_Prediction', 'Exited_Probability' å…©æ¬„çš„çµæœã€‚
        """
        logger.info(f"é–‹å§‹æ‰¹æ¬¡é æ¸¬ï¼Œå…± {len(input_df)} ç­†è³‡æ–™ã€‚")
        
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœå‹™æœªå•Ÿå‹•ï¼Œç„¡æ³•é€²è¡Œæ‰¹æ¬¡é æ¸¬ã€‚")

        # 1. ä¿å­˜åŸå§‹çš„ CustomerId (ç”¨æ–¼æœ€çµ‚çµæœ)
        customer_ids = input_df['CustomerId'] if 'CustomerId' in input_df.columns else range(len(input_df))
        
        # 2. ç‰¹å¾µå·¥ç¨‹
        processed_df = fe_pipeline_func(input_df.copy())
        
        # 3. OHE å’Œç‰¹å¾µå°é½Š
        X_predict = self._align_features(processed_df)
        
        logger.info(f"ç‰¹å¾µå°é½Šå¾Œï¼Œé æ¸¬æ•¸æ“šå½¢ç‹€: {X_predict.shape}")
        # 4. é€²è¡Œé æ¸¬
        probabilities = self.model.predict_proba(X_predict)[:, 1]
        
        predictions = (probabilities >= 0.5).astype(int)

        # 5. æ§‹å»ºçµæœ DataFrame (ä¿æŒåŸå§‹æ•¸æ“šï¼Œä¸¦æ·»åŠ çµæœ)
        result_df = pd.DataFrame({
        'CustomerId': customer_ids, # ä½¿ç”¨ CustomerId (å¤§å¯« D)
        'Exited_Prediction': predictions,
        'Exited_Probability': probabilities
    })

        
        # ç¢ºä¿ Column å‘½åæ¸…æ™°
        result_df['Exited_Probability'] = probabilities
        result_df['Exited_Prediction'] = predictions 
        
        logger.info(f"Service: æ‰¹æ¬¡é æ¸¬å®Œæˆï¼Œè¿”å›ç­†æ•¸: {len(result_df)}")
        return result_df
    

    def calculate_roi_batch(self, df_with_prob: pd.DataFrame) -> Dict[str, Any]:
        """
        åŸºæ–¼é æ¸¬çµæœè¨ˆç®— LTV èˆ‡ ROI (é‚è¼¯ä¾†è‡ª customer_churn_bank_roi.ipynb)
        """
        df = df_with_prob.copy()
        
        # --- 1. å®šç¾©å¸¸æ•¸ (ä¾†è‡ª Notebook) ---
        NIM_RATE = 0.02
        PRODUCT_PROFIT = 50.0
        ACTIVE_CARD_PROFIT = 30.0
        L_MAX = 10.0
        USER_RETENTION_COST = 500.0
        USER_SUCCESS_RATE = 0.20

        # --- 2. LTV è¨ˆç®— ---
        # ç¢ºä¿é¢¨éšªæ¬„ä½å­˜åœ¨ (Route å±¤å‚³å…¥æ™‚æ‡‰ç‚º 'Exited_Probability' æˆ– 'probability')
        prob_col = 'Exited_Probability' if 'Exited_Probability' in df.columns else 'probability'
        if prob_col not in df.columns:
            return {} # ç„¡æ³•è¨ˆç®—

        df['Churn_Prob'] = df[prob_col]
        
        # è¨ˆç®— ActiveCard_Flag
        df['ActiveCard_Flag'] = ((df['HasCrCard'] == 1) & (df['IsActiveMember'] == 1)).astype(int)

        # è¨ˆç®—å¹´åˆ©æ½¤
        df['Annual_Profit'] = (
            (df['Balance'] * NIM_RATE) +
            (df['NumOfProducts'] * PRODUCT_PROFIT) +
            (df['ActiveCard_Flag'] * ACTIVE_CARD_PROFIT)
        )

        # è¨ˆç®—é æœŸå£½å‘½ (é˜²æ­¢é™¤ä»¥ 0)
        df['Expected_Lifespan'] = np.minimum(1 / np.maximum(df['Churn_Prob'], 1e-6), L_MAX)
        
        # è¨ˆç®— LTV
        df['LTV'] = df['Annual_Profit'] * df['Expected_Lifespan']

        # --- 3. ROI æœ€ä½³åŒ–æ¨¡å‹ (Profit Ranking) ---
        # ENR = LTV * P(churn) * SR - RC
        df['ENR'] = (df['LTV'] * df['Churn_Prob'] * USER_SUCCESS_RATE) - USER_RETENTION_COST
        
        # ç¯©é¸å‡ºå€¼å¾—æŒ½ç•™çš„å®¢æˆ¶ (ENR > 0)
        actionable = df[df['ENR'] > 0].copy()
        actionable = actionable.sort_values(by='ENR', ascending=False)

        # --- 4. çµ±è¨ˆçµæœ ---
        total_ltv_all = df['LTV'].sum()
        actionable_count = len(actionable)
        total_enr = actionable['ENR'].sum() if not actionable.empty else 0.0
        total_cost = actionable_count * USER_RETENTION_COST
        
        # è¨ˆç®—æ•´é«” ROI
        total_roi = (total_enr / total_cost) if total_cost > 0 else 0.0

        return {
            'total_ltv': total_ltv_all,
            'actionable_count': actionable_count,
            'total_net_enr': total_enr,
            'retention_cost': total_cost,
            'total_roi': total_roi,
            # é€™è£¡ä¸å›å‚³ top_targetsï¼Œè®“å‰ç«¯ç´”é¡¯ç¤ºçµ±è¨ˆ
        }