# routes\customer_churn_bank_routes.py
import matplotlib
import matplotlib.font_manager as fm
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import logging
import base64
import shap
import sys
import os
import io

from flask import Blueprint, jsonify, request, send_file, make_response
from services.customer_churn_bank_service import CustomerChurnBankService
from typing import Any, Dict, List, Tuple, Callable
from werkzeug.exceptions import BadRequest
from config import Config

# è¨­ç½® Matplotlib ç‚ºéäº’å‹•å¼å¾Œç«¯ï¼Œç¢ºä¿åœ¨ä¼ºæœå™¨ç’°å¢ƒä¸­ç©©å®šé‹è¡Œ
matplotlib.use('Agg')

# --- å°ˆæ¡ˆè·¯å¾‘èˆ‡æ¨¡çµ„å°å…¥ ---
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(PROJECT_ROOT)

# --- æ—¥èªŒè¨­å®š ---
logger = logging.getLogger('CustomerChurnBankRoute')
logger.setLevel(logging.INFO)

logger.info("Matplotlib font cache cleanup logic has been removed for stability.")

# --- Matplotlib å…¨å±€è¨­å®š ---
plt.rcParams['axes.unicode_minus'] = False # ç¢ºä¿è² è™Ÿæ­£å¸¸é¡¯ç¤º

# --- æ¨¡å‹èˆ‡è³‡æºè·¯å¾‘å®šç¾© ---
# ç›´æ¥å¾ Config é¡åˆ¥ä¸­ç²å–å·²è¨ˆç®—å¥½çš„çµ•å°è·¯å¾‘
MODEL_PATH_FULL = Config.MODEL_BANK_PATH

# MODEL_DIR æ‡‰è©²æ˜¯æ¨¡å‹æª”æ¡ˆæ‰€åœ¨çš„ç›®éŒ„
MODEL_DIR = os.path.dirname(MODEL_PATH_FULL)

# å…¨å±€ SHAP æ‘˜è¦åœ–è·¯å¾‘ï¼Œç”¨æ–¼è¼‰å…¥é å…ˆè¨ˆç®—çš„å…¨å±€ç‰¹å¾µé‡è¦æ€§åœ–
GLOBAL_SHAP_FILE = os.path.join(MODEL_DIR, "shap_summary_plot.png")

# --- é æœŸæ ¸å¿ƒé æ¸¬ç‰¹å¾µåˆ—è¡¨ (å¿…é ˆå­˜åœ¨ä¸”æ•¸æ“šç„¡ç¼ºå¤±) ---
REQUIRED_PREDICT_COLUMNS = [
    'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts',
    'HasCrCard', 'IsActiveMember', 'EstimatedSalary',
    'Geography', 'Gender'
]

# --- å¿…é ˆå­˜åœ¨çš„æ¬„ä½ (ID + æ ¸å¿ƒé æ¸¬æ¬„ä½) ---
CRITICAL_COLUMNS = ['id'] + REQUIRED_PREDICT_COLUMNS


# --- é æœŸåŸå§‹ç‰¹å¾µåˆ—è¡¨ (åŒ…å«æ‰€æœ‰å¯é¸å’Œå¿…é ˆçš„æ¬„ä½) ---
REQUIRED_RAW_FEATURES = CRITICAL_COLUMNS + [
    'CustomerId', 'Surname', 'RowNumber'
]

# --- è¼”åŠ©å‡½å¼ï¼šè£œé½Šç¼ºå¤±æ¬„ä½ ---
def ensure_required_columns(df: pd.DataFrame, required_cols: List[str]) -> pd.DataFrame:
    """
    æª¢æŸ¥ä¸¦è£œé½Š DataFrame ä¸­ç¼ºå¤±çš„è¼”åŠ©æ¬„ä½ ('CustomerId', 'Surname', 'RowNumber')ã€‚
    
    ã€æ³¨æ„ã€‘: æ ¸å¿ƒæ¬„ä½ ('id' å’Œ REQUIRED_PREDICT_COLUMNS) çš„ç¼ºå¤±æ€§æª¢æŸ¥å·²åœ¨ predict_batch ä¸­å®Œæˆï¼Œ
             ä¸€æ—¦ç™¼ç¾ç¼ºå¤±æœƒç«‹å³æ‹‹å‡ºéŒ¯èª¤ï¼Œä¸æœƒé€²å…¥é€™è£¡ã€‚
    """
    df_copy = df.copy()
    
    # é€™è£¡åªå°ˆæ³¨æ–¼è™•ç†éæ ¸å¿ƒä½†å¯èƒ½éœ€è¦çš„æ¬„ä½ (CustomerId, RowNumber, Surname)
    auxiliary_cols = [col for col in required_cols if col not in CRITICAL_COLUMNS]
    missing_auxiliary_cols = set(auxiliary_cols) - set(df_copy.columns)
    
    # è™•ç† 'id' æ¬„ä½ï¼ˆé›–ç„¶åœ¨è·¯ç”±å±¤å·²æª¢æŸ¥ï¼Œé€™è£¡ç‚ºä¿éšªèµ·è¦‹å†ç¢ºä¿è™•ç†é¡å‹ï¼‰
    # ç¢ºä¿ 'id' å·²ç¶“å­˜åœ¨ä¸”é¡å‹æ­£ç¢º (æ­¤æ™‚ä¸æ‡‰æœ‰ NaN)
    if 'id' in df_copy.columns:
        df_copy['id'] = pd.to_numeric(df_copy['id'], errors='coerce').fillna(0).astype(int)
    
    if missing_auxiliary_cols:
        logger.warning(f"CSV æª”æ¡ˆä¸­ç¼ºå°‘ {len(missing_auxiliary_cols)} å€‹è¼”åŠ©æ¬„ä½ï¼Œå·²è‡ªå‹•è£œé½Š: {missing_auxiliary_cols}")
        
        sequential_id = df_copy.index.to_series() + 1
        
        for col in missing_auxiliary_cols:
            
            if col in ['CustomerId', 'RowNumber']:
                df_copy[col] = sequential_id
                
            elif col == 'Surname':
                df_copy[col] = ''
                
        # ç¢ºä¿é€™äº›è¼”åŠ© ID æ¬„ä½ä¹Ÿæ˜¯æ•´æ•¸
        for id_col in ['CustomerId', 'RowNumber']:
            if id_col in df_copy.columns:
                df_copy[id_col] = pd.to_numeric(df_copy[id_col], errors='coerce').fillna(0).astype(int)

    return df_copy


# --- ç‰¹å¾µå·¥ç¨‹é¡åˆ¥ (ä¿æŒä¸è®Š) ---
class FeatureEngineerForAPI:
    """ç”¨æ–¼å–®ä¸€æˆ–æ‰¹æ¬¡é æ¸¬å‰ï¼Œé€²è¡Œæ•¸æ“šæ¸…æ´—å’Œç‰¹å¾µè½‰æ›çš„é¡åˆ¥ã€‚"""
    @staticmethod
    def cast_columns(df: pd.DataFrame, int_cols: Any = None, cat_cols: Any = None) -> pd.DataFrame:
        """å°‡æŒ‡å®šæ¬„ä½è½‰æ›ç‚ºæ•´æ•¸ (int) æˆ–é¡åˆ¥ (category) é¡å‹ï¼Œè™•ç†ç¼ºå¤±å€¼ç‚º 0ã€‚"""
        df_copy = df.copy()
        if int_cols:
            for col in int_cols:
                if col in df_copy.columns:
                    # é€™è£¡å‡è¨­è¼¸å…¥æ•¸æ“šå·²ç¶“é NaN æª¢æŸ¥ï¼Œæ‰€ä»¥ fillna(0) è™•ç†çš„æ˜¯å¼·åˆ¶è½‰æ›å¼•èµ·çš„éŒ¯èª¤
                    df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce').fillna(0).astype(int)
        if cat_cols:
            for col in cat_cols:
                if col in df_copy.columns:
                    df_copy[col] = df_copy[col].astype('category')
        return df_copy

    @staticmethod
    def run_v1_preprocessing(df: pd.DataFrame) -> pd.DataFrame:
        """åŸ·è¡Œç¬¬ä¸€éšæ®µçš„ç‰¹å¾µå·¥ç¨‹ï¼šè™•ç†é¡åˆ¥æ˜ å°„ã€æ–°å¢åŸºç¤è¡ç”Ÿç‰¹å¾µã€‚"""
        df_copy = df.copy()
        
        # è™•ç† Gender (å°‡æ•¸å€¼ 0/1 è½‰æ›ç‚º Male/Female)
        if df_copy['Gender'].dtype in ['int64', 'float64']:
            df_copy['Gender'] = df_copy['Gender'].replace({0: 'Male', 1: 'Female'})
        df_copy['Gender'] = df_copy['Gender'].astype('category')

        # è™•ç† Geography (å°‡æ•¸å€¼ 0/1/2 è½‰æ›ç‚º France/Spain/Germany)
        geo_map = {0: 'France', 1: 'Spain', 2: 'Germany'}
        if df_copy['Geography'].dtype in ['int64', 'float64']:
            df_copy['Geography'] = df_copy['Geography'].replace(geo_map)
        df_copy['Geography'] = df_copy['Geography'].astype('category')

        # è¡ç”Ÿç‰¹å¾µï¼šAge åˆ†ç®±
        df_copy['Age_bin'] = pd.cut(df_copy['Age'], bins=[0, 25, 35, 45, 60, np.inf],
                                     labels=['very_young', 'young', 'mid', 'mature', 'senior'],
                                     right=False).astype('category')
        # è¡ç”Ÿç‰¹å¾µï¼šæ˜¯å¦æ“æœ‰ 2 å€‹ç”¢å“
        df_copy['Is_two_products'] = (df_copy['NumOfProducts'] == 2).astype(int)
        # è¡ç”Ÿç‰¹å¾µï¼šå¾·åœ‹å¥³æ€§ã€å¾·åœ‹éæ´»èºæœƒå“¡ã€é¤˜é¡ç‚ºé›¶ã€Tenure å–å°æ•¸
        df_copy['Germany_Female'] = ((df_copy['Geography'] == 'Germany') & (df_copy['Gender'] == 'Female')).astype(int)
        df_copy['Germany_Inactive'] = ((df_copy['Geography'] == 'Germany') & (df_copy['IsActiveMember'] == 0)).astype(int)
        df_copy['Has_Zero_Balance'] = (df_copy['Balance'] == 0).astype(int)
        df_copy['Tenure_log'] = np.log1p(df_copy['Tenure'])

        # è½‰æ›æ¬„ä½é¡å‹
        int_cols = ['HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Is_two_products',
                    'Has_Zero_Balance', 'Germany_Female', 'Germany_Inactive']
        cat_cols = ['Geography', 'Age_bin', 'Gender']
        df_copy = FeatureEngineerForAPI.cast_columns(df_copy, int_cols=int_cols, cat_cols=cat_cols)

        # ç§»é™¤ä¸å¿…è¦çš„æ¬„ä½
        cols_to_drop = ['CustomerId', 'Tenure', 'Surname', 'RowNumber']
        df_copy.drop(columns=[col for col in cols_to_drop if col in df_copy.columns], inplace=True, errors='ignore')

        return df_copy

    @staticmethod
    def run_v2_preprocessing(df: pd.DataFrame) -> pd.DataFrame:
        """åŸ·è¡Œç¬¬äºŒéšæ®µç‰¹å¾µå·¥ç¨‹ï¼šåœ¨ V1 åŸºç¤ä¸Šæ–°å¢æ›´è¤‡é›œçš„äº’å‹•ç‰¹å¾µã€‚"""
        df_copy = FeatureEngineerForAPI.run_v1_preprocessing(df.copy())
        # æ–°å¢äº’å‹•ç‰¹å¾µï¼šæˆç†Ÿã€éæ´»èºä¸”é¤˜é¡ç‚ºé›¶çš„å®¢æˆ¶
        df_copy['is_mature_inactive_transit'] = (
            (df_copy['Has_Zero_Balance'] == 1) & (df_copy['IsActiveMember'] == 0) & (df_copy['Age'] > 40)
        ).astype(int)
        return df_copy

# --- åœ–è¡¨ç”Ÿæˆè¼”åŠ©å‡½å¼ (ä¿æŒä¸è®Š) ---
def generate_local_shap_chart(shap_data: Dict[str, float], title: str) -> str:
    """
    ä½¿ç”¨ Matplotlib ç¹ªè£½å±€éƒ¨ SHAP å½±éŸ¿åŠ›æ°´å¹³æŸ±ç‹€åœ–ï¼Œä¸¦è½‰æ›ç‚º Base64 åœ–ç‰‡å­—ä¸²ã€‚
    ç”¨æ–¼è§£é‡‹å–®ä¸€é æ¸¬çš„ç‰¹å¾µè²¢ç»ã€‚
    """
    if not shap_data:
        logger.warning("SHAP data is empty, unable to draw chart.")
        return ""

    try:
        # æ ¹æ“š SHAP å€¼çš„çµ•å°å€¼é™åºæ’åˆ—
        sorted_data = dict(sorted(shap_data.items(), key=lambda item: abs(item[1]), reverse=True))
        
        features = list(sorted_data.keys())
        importances = list(sorted_data.values())

        # é¡è‰²è¨­ç½®ï¼šç´…è‰²æ¨é«˜æµå¤±ï¼Œç¶ è‰²æ¨ä½æµå¤±
        colors = ['#EF5350' if imp > 0 else '#66BB6A' for imp in importances]
        
        plt.style.use('seaborn-v0_8-whitegrid')
        
        fig, ax = plt.subplots(figsize=(10, len(features) * 0.7 + 1))
        
        ax.barh(features, importances, color=colors)
        
        ax.axvline(0, color='grey', linestyle='--', linewidth=0.8)

        ax.set_xlabel("SHAP Impact (Positive Pushes for Churn / Negative Against)")
        ax.set_title(title, fontsize=14)
        ax.invert_yaxis()

        # è½‰æ›ç‚º Base64
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight')
        plt.close(fig)
        
        return base64.b64encode(buf.getvalue()).decode('utf-8')

    except Exception as e:
        logger.error(f"Failed to generate local SHAP chart: {e}")
        return ""


# --- Service å¯¦ä¾‹åŒ–èˆ‡å…¨å±€è³‡æºè¼‰å…¥ (ä¿æŒä¸è®Š) ---
CUSTOMER_CHURN_BANK_SERVICE = None
GLOBAL_SHAP_BASE64 = "" # ç”¨æ–¼å„²å­˜é å…ˆè¼‰å…¥çš„å…¨å±€ SHAP åœ–

try:
    # æ‰“å°è·¯å¾‘ä¿¡æ¯
    logger.info(f"æ¨¡å‹è·¯å¾‘: {MODEL_PATH_FULL}")
    logger.info(f"æ¨¡å‹ç›®éŒ„: {MODEL_DIR}")
    logger.info(f"å…¨å±€ SHAP è·¯å¾‘: {GLOBAL_SHAP_FILE}")

    # 1. åˆå§‹åŒ–æ¨¡å‹æœå‹™
    CUSTOMER_CHURN_BANK_SERVICE = CustomerChurnBankService(
        model_path=MODEL_PATH_FULL,
        model_dir=MODEL_DIR
    )
    logger.info("CustomerChurnBankService æˆåŠŸåˆå§‹åŒ–ã€‚")

    # 2. è¼‰å…¥é›¢ç·šç”Ÿæˆçš„å…¨å±€ SHAP åœ–è¡¨
    if os.path.exists(GLOBAL_SHAP_FILE):
        with open(GLOBAL_SHAP_FILE, "rb") as f:
            GLOBAL_SHAP_BASE64 = base64.b64encode(f.read()).decode('utf-8')
        logger.info(f"å…¨å±€ SHAP æ‘˜è¦åœ– ({os.path.basename(GLOBAL_SHAP_FILE)}) è¼‰å…¥æˆåŠŸã€‚")
    else:
        logger.warning(f"å…¨å±€ SHAP åœ–è¡¨æª”æ¡ˆæœªæ‰¾åˆ°: {GLOBAL_SHAP_FILE}ã€‚ç„¡æ³•æä¾›å…¨å±€è§£é‡‹åœ–ã€‚")

except Exception as e:
    # é€™è£¡æ˜¯æœ€é—œéµçš„ä¿®æ­£ï¼šä¸åƒ…è¨˜éŒ„éŒ¯èª¤ï¼Œé‚„å°‡éŒ¯èª¤ä¿¡æ¯æ‰“å°å‡ºä¾†
    error_message = f"!!! åš´é‡éŒ¯èª¤ !!! åˆå§‹åŒ–æœå‹™æˆ–è¼‰å…¥å…¨å±€è³‡æºå¤±æ•—: {e}"
    logger.error(error_message, exc_info=True)
    
    # ç‚ºäº†ç¢ºä¿éŒ¯èª¤è¨Šæ¯èƒ½è¢«æ•æ‰ï¼Œæˆ‘å€‘åœ¨é€™è£¡å¼·åˆ¶è®“æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•å¤±æ•—ï¼Œä¸¦æ‰“å°éŒ¯èª¤è·¯å¾‘
    # é€™ä¸€è¡Œåœ¨ Production ä¸­æ‡‰è©²é¿å…ï¼Œä½†åœ¨è¨ºæ–·æ™‚éå¸¸æœ‰ç”¨
    # æª¢æŸ¥æ˜¯å¦ç‚ºFileNotFoundError
    if isinstance(e, FileNotFoundError):
        logger.error(f"è·¯å¾‘éŒ¯èª¤ï¼šæ¨¡å‹æˆ–è³‡æºæª”æ¡ˆæœªæ‰¾åˆ°ã€‚æª¢æŸ¥è·¯å¾‘ï¼š{MODEL_PATH_FULL} æˆ– {GLOBAL_SHAP_FILE}")
        # ç‚ºäº†è®“ Gunicorn/Render æ•æ‰åˆ°éŒ¯èª¤ï¼Œé‡æ–°æ‹‹å‡ºç•°å¸¸
        raise RuntimeError(f"æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œæª”æ¡ˆè·¯å¾‘éŒ¯èª¤ï¼š{e}") from e
    
    # å°å…¶ä»–éŒ¯èª¤ä¹Ÿå¼·åˆ¶æ‹‹å‡º
    raise RuntimeError(f"æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼š{e}") from e

# --- Blueprint å®šç¾© ---
customer_churn_bank_blueprint = Blueprint('customer_churn_bank_blueprint', __name__)

# -----------------------------------------------------------------------

## ğŸ“ˆ å–®ä¸€å®¢æˆ¶æµå¤±é æ¸¬ API (ä¿æŒä¸è®Š)
@customer_churn_bank_blueprint.route('/predict', methods=['POST'])
def predict_churn():
    """
    æ¥æ”¶å–®ä¸€å®¢æˆ¶çš„ JSON è¼¸å…¥ï¼Œé€²è¡Œé æ¸¬ã€å±€éƒ¨ SHAP åˆ†æï¼Œä¸¦è¿”å›çµæœã€‚
    """
    try:
        data = request.get_json()
        if not data:
            raise BadRequest("ç„¡æ•ˆçš„ JSON è«‹æ±‚")

        # 1. æ•´ç†è¼¸å…¥æ•¸æ“šä¸¦ä½¿ç”¨é è¨­å€¼
        input_data = {
            'id': 0, 
            'CreditScore': float(data.get('CreditScore', 650)),
            'Age': float(data.get('Age', 40)),
            'Tenure': float(data.get('Tenure', 5)),
            'Balance': float(data.get('Balance', 0)),
            'NumOfProducts': float(data.get('NumOfProducts', 1)),
            'HasCrCard': float(data.get('HasCrCard', 1)),
            'IsActiveMember': float(data.get('IsActiveMember', 1)),
            'EstimatedSalary': float(data.get('EstimatedSalary', 100000)),
            'Geography': float(data.get('Geography', 0)), 
            'Gender': float(data.get('Gender', 0)), 
            'CustomerId': 0,
            'Surname': 'A',
            'RowNumber': 0
        }

        input_df = pd.DataFrame([input_data])
        
        proba_churn = 0.5
        chart_base64_local = ""
        feature_importance_text = "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡æ“¬é æ¸¬ï¼Œç„¡æ³•æä¾› AI è§£é‡‹ã€‚"
        final_charts = []

        if CUSTOMER_CHURN_BANK_SERVICE and CUSTOMER_CHURN_BANK_SERVICE.model:
            # 2. å‘¼å«æœå‹™å±¤é€²è¡Œé è™•ç†ã€é æ¸¬å’Œ SHAP åˆ†æ
            prediction_results = CUSTOMER_CHURN_BANK_SERVICE.preprocess_and_predict(
                input_df=input_df, 
                fe_pipeline_func=FeatureEngineerForAPI.run_v2_preprocessing
            )
            
            proba_churn = prediction_results['probability']
            feature_importance_text = prediction_results['feature_importance']
            local_shap_values = prediction_results['local_shap_values']
            
            # 3. ç¹ªè£½å±€éƒ¨ SHAP åœ–è¡¨
            chart_base64_local = generate_local_shap_chart(
                local_shap_values, 
                f"Individual SHAP Local Influence (Churn Probability: {proba_churn:.4f})"
            )
            
            # 4. åŠ å…¥å…¨å±€ SHAP åœ–è¡¨ (å¦‚æœå·²è¼‰å…¥)
            if GLOBAL_SHAP_BASE64:
                final_charts.append({
                    "type": "image/png", 
                    "base64_data": GLOBAL_SHAP_BASE64,
                    "title": "æ¨¡å‹å…¨å±€ SHAP ç‰¹å¾µåœ– (æ•´é«”ç‰¹å¾µé‡è¦æ€§)"
                })

            # 5. çµ„è£å±€éƒ¨åœ–è¡¨çµæœ
            if chart_base64_local:
                final_charts.append({
                    "type": "image/png", 
                    "base64_data": chart_base64_local,
                    "title": f"å–®ç­†å®¢æˆ¶ SHAP ç‰¹å¾µåˆ†æ æµå¤±æ©Ÿç‡ : {proba_churn:.4f}"
                })
                
            # 6. è™•ç†å¯è®€æ€§è¼¸å‡º
            geography_map = {0: "æ³•åœ‹ (France)", 1: "è¥¿ç­ç‰™ (Spain)", 2: "å¾·åœ‹ (Germany)"}
            gender_map = {0: "ç”·æ€§ (Male)", 1: "å¥³æ€§ (Female)"}
            readable_data = {
                'ä¿¡ç”¨åˆ†æ•¸': data.get('CreditScore', 0),
                'å¹´é½¡': data.get('Age', 0),
                'æœå‹™å¹´é™': data.get('Tenure', 0),
                'é¤˜é¡': f"${float(data.get('Balance',0)):.2f}",
                'ç”¢å“æ•¸é‡': data.get('NumOfProducts', 0),
                'æŒæœ‰ä¿¡ç”¨å¡': "æ˜¯" if data.get('HasCrCard', 0) == 1 else "å¦",
                'æ´»èºæœƒå“¡': "æ˜¯" if data.get('IsActiveMember', 0) == 1 else "å¦",
                'ä¼°è¨ˆè–ªè³‡': f"${float(data.get('EstimatedSalary',0)):.2f}",
                'åœ‹å®¶/åœ°å€': geography_map.get(data.get('Geography', -1), 'æœªçŸ¥'),
                'æ€§åˆ¥': gender_map.get(data.get('Gender', -1), 'æœªçŸ¥')
            }
            
            explanation_prompt_snippet = f"æ¨¡å‹é æ¸¬çš„å®¢æˆ¶æµå¤±æ©Ÿç‡ç‚º {proba_churn:.4f}ã€‚\né—œéµç‰¹å¾µè³‡è¨Š:\n{feature_importance_text}"
            
            # 7. è¿”å›çµæœ
            return jsonify({
                "status": "success",
                "prediction": float(proba_churn),
                "readable_features": readable_data, 
                "explanation_prompt": explanation_prompt_snippet, 
                "charts": final_charts
            })

        # æ¨¡æ“¬çµæœçš„è¿”å›
        readable_data = {
            'ä¿¡ç”¨åˆ†æ•¸': data.get('CreditScore', 0),
            # ... (å…¶ä»–å¯è®€æ€§æ•¸æ“š)
        }
        return jsonify({
            "status": "warning",
            "prediction": 0.5,
            "readable_features": readable_data, 
            "explanation_prompt": "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡æ“¬é æ¸¬ï¼Œç„¡æ³•æä¾› AI è§£é‡‹ã€‚", 
            "charts": []
        })

    except BadRequest as e:
        logger.error(f"API è«‹æ±‚éŒ¯èª¤: {e}")
        return jsonify({"error": str(e)}), 400
    except ValueError as e:
        logger.error(f"æ•¸æ“šè™•ç†éŒ¯èª¤: {e}")
        return jsonify({"error": f"æ•¸æ“šè™•ç†å¤±æ•—: {e}"}), 400
    except Exception as e:
        logger.error(f"é æ¸¬éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        return jsonify({"error": f"ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}"}), 500

## ğŸ’¾ æ‰¹æ¬¡å®¢æˆ¶æµå¤±é æ¸¬ API
@customer_churn_bank_blueprint.route('/predict_batch', methods=['POST'])
def predict_batch():
    """
    æ¥æ”¶ CSV æª”æ¡ˆä¸Šå‚³ï¼Œé€²è¡Œæ‰¹æ¬¡æµå¤±é æ¸¬ï¼Œä¸¦è¿”å›çµæœ JSON æ•¸æ“šã€‚
    - åš´æ ¼æª¢æŸ¥ CRITICAL_COLUMNS (id + 10æ ¸å¿ƒç‰¹å¾µ) æ˜¯å¦å­˜åœ¨ä¸”æ•¸æ“šç„¡ä»»ä½•ç¼ºå¤±ã€‚
    - åªè¦æœ‰ä»»ä½•ç¼ºå¤±ï¼Œå³æ‹’çµ•æ•´å€‹ CSV æª”æ¡ˆå°å…¥ã€‚
    """
    logger.info("æ¥æ”¶åˆ°æ‰¹æ¬¡é æ¸¬è«‹æ±‚ã€‚")
    if CUSTOMER_CHURN_BANK_SERVICE is None or CUSTOMER_CHURN_BANK_SERVICE.model is None:
        logger.error("æ¨¡å‹æœå‹™æœªå•Ÿå‹•ï¼Œç„¡æ³•é€²è¡Œæ‰¹æ¬¡é æ¸¬ã€‚")
        return jsonify({"error": "æ¨¡å‹æœå‹™æœªå•Ÿå‹•ï¼Œç„¡æ³•é€²è¡Œæ‰¹æ¬¡é æ¸¬ã€‚"}), 503

    # 1. æª”æ¡ˆæª¢æŸ¥
    if 'file' not in request.files:
        raise BadRequest("è«‹æ±‚ä¸­æœªåŒ…å«æª”æ¡ˆã€‚è«‹ä¸Šå‚³ CSV æª”æ¡ˆã€‚")
    
    file = request.files['file']

    if not file.filename:
        raise BadRequest("æœªé¸æ“‡æª”æ¡ˆæˆ–æª”æ¡ˆåç„¡æ•ˆã€‚")

    if not file.filename.lower().endswith('.csv'):
        raise BadRequest("æª”æ¡ˆæ ¼å¼éŒ¯èª¤ã€‚è«‹ä¸Šå‚³ CSV æª”æ¡ˆã€‚")

    try:
        # 2. è®€å– CSV æª”æ¡ˆè‡³ DataFrame
        # keep_default_na=True ç¢ºä¿æ¨™æº–ç¼ºå¤±å€¼è¢«è®€å–ç‚º NaN
        data_io = io.StringIO(file.read().decode('utf-8'))
        input_df_original = pd.read_csv(data_io, keep_default_na=True, na_values=['', 'NA', 'N/A'])
        
        if input_df_original.empty:
            raise ValueError("CSV æª”æ¡ˆç‚ºç©ºã€‚")
            
        # ------------------------------------------------------------------
        # â˜…â˜…â˜… çµæ§‹æ€§æª¢æŸ¥ï¼šæª¢æŸ¥æ ¸å¿ƒæ¬„ä½æ˜¯å¦å­˜åœ¨ (Fail Fast) â˜…â˜…â˜…
        # ------------------------------------------------------------------
        missing_cols = [col for col in CRITICAL_COLUMNS if col not in input_df_original.columns]
        if missing_cols:
            error_msg = f"CSV æª”æ¡ˆä¸­ç¼ºå°‘é—œéµæ¬„ä½ï¼Œç„¡æ³•å°å…¥ã€‚ç¼ºå¤±æ¬„ä½: {', '.join(missing_cols)}"
            logger.error(f"çµæ§‹æ€§æª¢æŸ¥å¤±æ•—: {error_msg}")
            return jsonify({"error": error_msg}), 400
        
        # --------------------------------------------------------------
        # â˜…â˜…â˜… æ•¸æ“šæª¢æŸ¥ï¼šæª¢æŸ¥é—œéµæ¬„ä½ä¸­æ˜¯å¦å­˜åœ¨ä»»ä½• NaN å€¼ (Fail Fast) â˜…â˜…â˜…
        # --------------------------------------------------------------
        # ç¯©é¸å‡ºé—œéµæ¬„ä½çš„å­é›†
        df_critical = input_df_original[CRITICAL_COLUMNS]
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½• NaN å€¼
        if df_critical.isnull().values.any():
            # å®šä½ç¼ºå¤±å€¼æ‰€åœ¨çš„æ¬„ä½
            missing_data_cols = df_critical.columns[df_critical.isnull().any()].tolist()
            
            error_msg = f"CSV æª”æ¡ˆåœ¨é—œéµæ¬„ä½ä¸­ç™¼ç¾ç¼ºå¤±å€¼ï¼Œç„¡æ³•å°å…¥ã€‚åŒ…å«ç¼ºå¤±å€¼çš„æ¬„ä½: {', '.join(missing_data_cols)}"
            logger.error(f"æ•¸æ“šç¼ºå¤±æª¢æŸ¥å¤±æ•—: {error_msg}")
            return jsonify({"error": error_msg}), 400
        
        logger.info("çµæ§‹å’Œæ•¸æ“šç¼ºå¤±æ€§æª¢æŸ¥é€šéã€‚")
        
        # 3. è£œé½Šéæ ¸å¿ƒæ¬„ä½ ('CustomerId', 'RowNumber', 'Surname')
        input_df_processed = ensure_required_columns(input_df_original, REQUIRED_RAW_FEATURES)
        
        logger.info(f"æ‰¹æ¬¡é æ¸¬ - è¼”åŠ©æ•¸æ“šè£œé½Šå®Œæˆã€‚æ•¸æ“šç­†æ•¸: {len(input_df_processed)}")
        
        # 4. å‘¼å«æœå‹™å±¤é€²è¡Œæ‰¹æ¬¡é æ¸¬
        result_df = CUSTOMER_CHURN_BANK_SERVICE.predict_batch_csv(
            input_df=input_df_processed, 
            fe_pipeline_func=FeatureEngineerForAPI.run_v2_preprocessing
        )
        
        # 5. æº–å‚™ JSON å›æ‡‰
        
        # é¸æ“‡è¦è¿”å›çš„åŸå§‹ç‰¹å¾µæ¬„ä½
        # åŒ…å« 10 å€‹æ ¸å¿ƒç‰¹å¾µ + id (å…± 11 å€‹æ¬„ä½)
        feature_cols_to_return = [
            'id', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 
            'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'
        ]
        
        # ç¢ºä¿åªæœ‰åœ¨ CSV æª”ä¸­å­˜åœ¨çš„æ¬„ä½è¢«é¸å–
        available_cols = [col for col in feature_cols_to_return if col in input_df_processed.columns]
        
        # åˆä½µåŸå§‹ç‰¹å¾µå’Œé æ¸¬çµæœ
        result_df_full = input_df_processed[available_cols].copy()
        result_df_full['probability'] = result_df['Exited_Probability']
        
        # é—œéµï¼šè™•ç† NaN å€¼ã€å››æ¨äº”å…¥å’Œè³‡æ–™é¡å‹è½‰æ›ï¼Œé¿å… JSON åºåˆ—åŒ–éŒ¯èª¤
        for col in ['id', 'NumOfProducts', 'HasCrCard', 'IsActiveMember']:
             if col in result_df_full.columns:
                 result_df_full[col] = result_df_full[col].fillna(0).astype(int)

        for col in ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary', 'probability']:
             if col in result_df_full.columns:
                 # ä¿ç•™å°æ•¸é»å¾Œå…©ä½ï¼Œä¸¦è™•ç† NaN
                 result_df_full[col] = result_df_full[col].fillna(0.0).astype(float).round(2) 
        
        # è½‰æ›ç‚ºå‰ç«¯æ‰€éœ€çš„ JSON åˆ—è¡¨æ ¼å¼
        result_list = result_df_full.to_dict('records')
        
        # 6. è¿”å›çµæœ
        return jsonify({
            "status": "success",
            "message": f"æˆåŠŸé æ¸¬ {len(result_list)} ç­†è³‡æ–™ã€‚",
            "data": result_list
        })

    except BadRequest as e:
        logger.error(f"æ‰¹æ¬¡ API è«‹æ±‚éŒ¯èª¤: {e}")
        return jsonify({"error": str(e)}), 400
    except ValueError as e:
        # æ•ç² ensure_required_columns æ‹‹å‡ºçš„ id ç¼ºå¤±éŒ¯èª¤ æˆ– CSV ç‚ºç©ºéŒ¯èª¤
        logger.error(f"æ‰¹æ¬¡æ•¸æ“šè™•ç†éŒ¯èª¤ (CSV å…§å®¹): {e}")
        return jsonify({"error": f"CSV å…§å®¹æ ¼å¼éŒ¯èª¤: {e}"}), 400
    except RuntimeError as e:
        logger.error(f"æ¨¡å‹é æ¸¬å¤±æ•—: {e}")
        return jsonify({"error": str(e)}), 503
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡é æ¸¬éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        return jsonify({"error": f"ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}"}), 500